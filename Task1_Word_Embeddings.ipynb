{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VZXi_KGi0UR"
   },
   "source": [
    "# Task 1: Word Embeddings (10 points)\n",
    "\n",
    "This notebook will guide you through all steps necessary to train a word2vec model (Detailed description in the PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48t-II1vkuau"
   },
   "source": [
    "## Imports\n",
    "\n",
    "This code block is reserved for your imports. \n",
    "\n",
    "You are free to use the following packages: \n",
    "\n",
    "(List of packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4kh6nh84-AOL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch as torch\n",
    "from torch import nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math as mt\n",
    "import time\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import DataLoader\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWmk3hVllEcU"
   },
   "source": [
    "# 1.1 Get the data (0.5 points)\n",
    "\n",
    "The Hindi portion HASOC corpus from [github.io](https://hasocfire.github.io/hasoc/2019/dataset.html) is already available in the repo, at data/hindi_hatespeech.tsv . Load it into a data structure of your choice. Then, split off a small part of the corpus as a development set (~100 data points).\n",
    "\n",
    "If you are using Colab the first two lines will let you upload folders or files from your local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XtI7DJ-0-AOP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "    \n",
    "url = 'https://raw.githubusercontent.com/salmanedhi/NNTI-WS2021-NLP-Project/main/data/hindi_hatespeech.tsv'\n",
    "\n",
    "print(\"Done\", dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hate       category\n",
      "11775  সাকিব ভাইয়ের জন্য কষ্ট হচ্ছে সাকিব ছাড়া বাংল...     0         sports\n",
      "10373                 বাংলাদেশ হচ্ছে আগামী দিনের ভবিষ্যৎ     0         sports\n",
      "11389  জারা জারা আমার মতো ভাইকে ভালোবাসেন মন থেকে সুধ...     0         sports\n",
      "10330                                      আমি অনেক হেপি     0         sports\n",
      "100    রুবেল হেপীরে লাগাইয়া জাইরা দিচোছ,,সালা মরলে বো...     1         sports\n",
      "...                                                  ...   ...            ...\n",
      "1794   পরিচারক কুত্তার বাচ্চা মুনাজাত নিয়ে মঝা করছ,, ...     1  entertainment\n",
      "11196                                     আপনার কথা রাইট     0         sports\n",
      "12273  শোয়েব মালিক তোর বাপের চেয়ে বয়সে বড় শালা। শোয়েব...     0         sports\n",
      "10450                     সাব্বির ভাই তুই সেরা তুই পারবি     0         sports\n",
      "173                               বলদ কোনুদিন মানোষ হয়না     1         sports\n",
      "\n",
      "[4665 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Reading Bengali data\n",
    "bengali_data = pd.read_csv('data/bengali_hatespeech.csv')\n",
    "\n",
    "bengali_data_hate = bengali_data.loc[bengali_data['hate'] == 1]\n",
    "bengali_data_not_hate = bengali_data.loc[bengali_data['hate'] == 0]\n",
    "\n",
    "bengali_data_hate = bengali_data_hate.iloc[0:2332] \n",
    "bengali_data_not_hate = bengali_data_not_hate.iloc[0:2333]\n",
    "\n",
    "bengali_data = pd.concat([bengali_data_hate, bengali_data_not_hate])\n",
    "bengali_data = shuffle(bengali_data)\n",
    "bengali_data.columns = [\"text\", \"hate\", \"category\"]\n",
    "\n",
    "labels = bengali_data['hate']\n",
    "labels = np.array(labels)\n",
    "\n",
    "data_development = bengali_data \n",
    "print(bengali_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-mSJ8nUlupB"
   },
   "source": [
    "## 1.2 Data preparation (0.5 + 0.5 points)\n",
    "\n",
    "* Prepare the data by removing everything that does not contain information. \n",
    "User names (starting with '@') and punctuation symbols clearly do not convey information, but we also want to get rid of so-called [stopwords](https://en.wikipedia.org/wiki/Stop_word), i. e. words that have little to no semantic content (and, but, yes, the...). Hindi stopwords can be found [here](https://github.com/stopwords-iso/stopwords-hi/blob/master/stopwords-hi.txt) Then, standardize the spelling by lowercasing all words.\n",
    "Do this for the development section of the corpus for now.\n",
    "\n",
    "* What about hashtags (starting with '#') and emojis? Should they be removed too? Justify your answer in the report, and explain how you accounted for this in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-386ff909cd20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the Drive helper and mount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# This will prompt for authorization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "\n",
    "# This will prompt for authorization.\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CHcNeyKi-AOQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#TODO: implement!\n",
    "# file = '/content/drive/My Drive/stopwords-hi.txt'\n",
    "\n",
    "# with open(file, 'r', encoding=\"utf8\") as f2:\n",
    "#     hindi_stopword_file = f2.read()\n",
    "    \n",
    "    \n",
    "hindi_stopword_file = open('data/stopwords-hi.txt', encoding=\"utf8\")\n",
    "\n",
    "sw_list = ['#', '?', '!', ';', ',', ':', \"\\'\", '-', '=', '(', ')', '[', ']' , '{', '}', '\"', '*', '@', '  ', '\\\\', '/', '..', '...', '....', '%'\n",
    "          ,'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '\\t']\n",
    "sw_list_string = ''\n",
    "for i in sw_list:\n",
    "    sw_list_string+=i\n",
    "hindi_stopwords = []\n",
    "for x in hindi_stopword_file:\n",
    "    hindi_stopwords.append(x.rstrip())\n",
    "\n",
    "hindi_stopwords.extend(sw_list)\n",
    "sentences = []\n",
    "for text in data_development['text']:\n",
    "    text_array = text.split(' ')\n",
    "    new_array = []\n",
    "    for j in text_array:\n",
    "        if '@' not in j and len(j) < 20:\n",
    "            for char in sw_list:\n",
    "                j = j.replace(char, '')\n",
    "            new_array.append(j.lower())\n",
    "    sentences.append(' '.join(new_array))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Je09nozLmmMm"
   },
   "source": [
    "## 1.3 Build the vocabulary (0.5 + 0.5 points)\n",
    "\n",
    "The input to the first layer of word2vec is an one-hot encoding of the current word. The output od the model is then compared to a numeric class label of the words within the size of the skip-gram window. Now\n",
    "\n",
    "* Compile a list of all words in the development section of your corpus and save it in a variable ```V```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VpoGmTKx-AOQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 64027\n",
      "Unique words: 14482\n"
     ]
    }
   ],
   "source": [
    "#TODO: implement!\n",
    "temp_unique = [] # For unique words\n",
    "temp_nounique = []\n",
    "for j in sentences:\n",
    "    temp2 = j.split(' ')\n",
    "    for k in temp2:\n",
    "        if k not in temp_unique:\n",
    "            temp_unique.append(k)\n",
    "        temp_nounique.append(k)\n",
    "V = temp_unique\n",
    "non_unique = temp_nounique\n",
    "print('Total words:', len(non_unique))\n",
    "print('Unique words:', len(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiaVglVNoENY"
   },
   "source": [
    "* Then, write a function ```word_to_one_hot``` that returns a one-hot encoding of an arbitrary word in the vocabulary. The size of the one-hot encoding should be ```len(v)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "yqPNw6IT-AOQ"
   },
   "outputs": [],
   "source": [
    "#TODO: implement!\n",
    "def word_to_one_hot(word, features):\n",
    "    if word in V:\n",
    "        index = V.index(word)\n",
    "        encoding = np.zeros(features)\n",
    "        encoding[index]= 1\n",
    "        return encoding.astype(np.uint8)\n",
    "    return False\n",
    "\n",
    "def not_word_to_one_hot(word, features):\n",
    "    if word in V:\n",
    "        index = V.index(word)\n",
    "#         encoding = np.zeros(features)\n",
    "#         encoding[index]= 1\n",
    "        return index\n",
    "    return False\n",
    "#   pass\n",
    "\n",
    "def index_to_onehot(X_batch):\n",
    "    X_batch_new = []\n",
    "    y_batch_new = []\n",
    "    encodingX = np.eye(4)[X_batch]\n",
    "    return type(encodingX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKD8zBlxVclh"
   },
   "source": [
    "## 1.4 Subsampling (0.5 points)\n",
    "\n",
    "The probability to keep a word in a context is given by:\n",
    "\n",
    "$P_{keep}(w_i) = \\Big(\\sqrt{\\frac{z(w_i)}{0.001}}+1\\Big) \\cdot \\frac{0.001}{z(w_i)}$\n",
    "\n",
    "Where $z(w_i)$ is the relative frequency of the word $w_i$ in the corpus. Now,\n",
    "* Calculate word frequencies\n",
    "* Define a function ```sampling_prob``` that takes a word (string) as input and returns the probabiliy to **keep** the word in a context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Mj4sDOVMMr0b"
   },
   "outputs": [],
   "source": [
    "#TODO: implement!\n",
    "def sampling_prob(word):\n",
    "    z_wi = non_unique.count(word) / len(non_unique)\n",
    "    try:\n",
    "        p_wi = (mt.sqrt(z_wi / 0.001) + 1) * (0.001 / z_wi)\n",
    "        return p_wi\n",
    "    except ZeroDivisionError:\n",
    "        print(\"Word doesn't exist in corpus\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14482,)\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "sampling_prob('সাদা')\n",
    "sample_onehot = word_to_one_hot('সাদা', len(V))\n",
    "print(sample_onehot.shape)\n",
    "print(sample_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxV1P90zplxu"
   },
   "source": [
    "# 1.5 Skip-Grams (1 point)\n",
    "\n",
    "Now that you have the vocabulary and one-hot encodings at hand, you can start to do the actual work. The skip gram model requires training data of the shape ```(current_word, context)```, with ```context``` being the words before and/or after ```current_word``` within ```window_size```. \n",
    "\n",
    "* Have closer look on the original paper. If you feel to understand how skip-gram works, implement a function ```get_target_context``` that takes a sentence as input and [yield](https://docs.python.org/3.9/reference/simple_stmts.html#the-yield-statement)s a ```(current_word, context)```.\n",
    "\n",
    "* Use your ```sampling_prob``` function to drop words from contexts as you sample them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "r8CCTpVy-AOR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#TODO: implement!\n",
    "def get_target_context(sentence, window_size):\n",
    "    word = ''\n",
    "    words_in_sentence = sentence.split(' ')\n",
    "    for i in range(len(words_in_sentence)):\n",
    "        context = []\n",
    "        randd = random.random()\n",
    "#         print('1st: ', words_in_sentence[i], sampling_prob(words_in_sentence[i]), randd)\n",
    "        if randd <= sampling_prob(words_in_sentence[i]):\n",
    "            word = words_in_sentence[i]\n",
    "            upper_bound = i + window_size + 1\n",
    "            lower_bound = i - window_size\n",
    "            for j in range(lower_bound, upper_bound):\n",
    "                rand2 = random.random()\n",
    "                if i != j and j>=0 and j<len(words_in_sentence):\n",
    "                    if rand2 <= sampling_prob(words_in_sentence[j]):\n",
    "#                     print('2nd: ', words_in_sentence[i], words_in_sentence[j],sampling_prob(words_in_sentence[j]), rand2)\n",
    "                        context.append(words_in_sentence[j])\n",
    "        if len(word) > 0 and len(context) > 0:\n",
    "            yield(word, context)\n",
    "            \n",
    "hello = get_target_context(sentences[22], 4)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfEFgtkmuDjL"
   },
   "source": [
    "# 1.6 Hyperparameters (0.5 points)\n",
    "\n",
    "According to the word2vec paper, what would be a good choice for the following hyperparameters? \n",
    "\n",
    "* Embedding dimension\n",
    "* Window size\n",
    "\n",
    "Initialize them in a dictionary or as independent variables in the code block below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d7xSKuFJcYoD"
   },
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "# window_size = from 1 to 10\n",
    "# window_size = from 500 to 1000 (640 used in paper)\n",
    "window_size = 4\n",
    "embedding_size = 600\n",
    "\n",
    "# More hyperparameters\n",
    "learning_rate = 0.05\n",
    "epochs =100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xiM2zq-YunPx"
   },
   "source": [
    "# 1.7 Pytorch Module (0.5 + 0.5 + 0.5 points)\n",
    "\n",
    "Pytorch provides a wrapper for your fancy and super-complex models: [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). The code block below contains a skeleton for such a wrapper. Now,\n",
    "\n",
    "* Initialize the two weight matrices of word2vec as fields of the class.\n",
    "\n",
    "* Override the ```forward``` method of this class. It should take a one-hot encoding as input, perform the matrix multiplications, and finally apply a log softmax on the output layer.\n",
    "\n",
    "* Initialize the model and save its weights in a variable. The Pytorch documentation will tell you how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9sGNytYhwxS",
    "outputId": "41645b64-e4ed-4e6a-e10f-74cb39b92230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Create model \n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, features, embedding_size):\n",
    "        super().__init__()\n",
    "        initrange = 0.5 / embedding_size\n",
    "        self.fc1 = nn.Linear(features, embedding_size)\n",
    "        self.fc2 = nn.Linear(embedding_size, features)\n",
    "\n",
    "\n",
    "    def forward(self, one_hot):\n",
    "        x = self.fc1(one_hot.float())\n",
    "        x = self.fc2(x)\n",
    "        log_softmax = torch.nn.functional.log_softmax(x, dim=1)\n",
    "        return log_softmax\n",
    "#     pass\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XefIDMMHv5zJ"
   },
   "source": [
    "# 1.8 Loss function and optimizer (0.5 points)\n",
    "\n",
    "Initialize variables with [optimizer](https://pytorch.org/docs/stable/optim.html#module-torch.optim) and loss function. You can take what is used in the word2vec paper, but you can use alternative optimizers/loss functions if you explain your choice in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating target, context tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4665\n",
      "started\n",
      "INDEX: 49 Length: 4354\n",
      "INDEX: 99 Length: 7803\n",
      "INDEX: 149 Length: 14806\n",
      "INDEX: 199 Length: 19318\n",
      "INDEX: 249 Length: 23037\n",
      "INDEX: 299 Length: 26081\n",
      "INDEX: 349 Length: 29032\n",
      "INDEX: 399 Length: 33300\n",
      "INDEX: 449 Length: 37392\n",
      "INDEX: 499 Length: 40806\n",
      "INDEX: 549 Length: 43409\n",
      "INDEX: 599 Length: 46910\n",
      "INDEX: 649 Length: 50287\n",
      "INDEX: 699 Length: 53414\n",
      "INDEX: 749 Length: 57366\n",
      "INDEX: 799 Length: 61557\n",
      "INDEX: 849 Length: 66592\n",
      "INDEX: 899 Length: 70255\n",
      "INDEX: 949 Length: 74136\n",
      "INDEX: 999 Length: 78724\n",
      "INDEX: 1049 Length: 82233\n",
      "INDEX: 1099 Length: 86720\n",
      "INDEX: 1149 Length: 89535\n",
      "INDEX: 1199 Length: 93427\n",
      "INDEX: 1249 Length: 97764\n",
      "INDEX: 1299 Length: 102196\n",
      "INDEX: 1349 Length: 107099\n",
      "INDEX: 1399 Length: 112144\n",
      "INDEX: 1449 Length: 115209\n",
      "INDEX: 1499 Length: 119113\n",
      "INDEX: 1549 Length: 122807\n",
      "INDEX: 1599 Length: 126594\n",
      "INDEX: 1649 Length: 130561\n",
      "INDEX: 1699 Length: 134107\n",
      "INDEX: 1749 Length: 137777\n",
      "INDEX: 1799 Length: 141392\n",
      "INDEX: 1849 Length: 145310\n",
      "INDEX: 1899 Length: 149330\n",
      "INDEX: 1949 Length: 152536\n",
      "INDEX: 1999 Length: 156173\n",
      "INDEX: 2049 Length: 161022\n",
      "INDEX: 2099 Length: 165269\n",
      "INDEX: 2149 Length: 173689\n",
      "INDEX: 2199 Length: 177304\n",
      "INDEX: 2249 Length: 180436\n",
      "INDEX: 2299 Length: 183668\n",
      "INDEX: 2349 Length: 188194\n",
      "INDEX: 2399 Length: 192324\n",
      "INDEX: 2449 Length: 195658\n",
      "INDEX: 2499 Length: 198738\n",
      "INDEX: 2549 Length: 202544\n",
      "INDEX: 2599 Length: 205148\n",
      "INDEX: 2649 Length: 208905\n",
      "INDEX: 2699 Length: 212284\n",
      "INDEX: 2749 Length: 215159\n",
      "INDEX: 2799 Length: 218864\n",
      "INDEX: 2849 Length: 224335\n",
      "INDEX: 2899 Length: 227593\n",
      "INDEX: 2949 Length: 233076\n",
      "INDEX: 2999 Length: 236045\n",
      "INDEX: 3049 Length: 239852\n",
      "INDEX: 3099 Length: 242790\n",
      "INDEX: 3149 Length: 245940\n",
      "INDEX: 3199 Length: 249324\n",
      "INDEX: 3249 Length: 253503\n",
      "INDEX: 3299 Length: 256908\n",
      "INDEX: 3349 Length: 260369\n",
      "INDEX: 3399 Length: 264882\n",
      "INDEX: 3449 Length: 270211\n",
      "INDEX: 3499 Length: 273218\n",
      "INDEX: 3549 Length: 276807\n",
      "INDEX: 3599 Length: 280767\n",
      "INDEX: 3649 Length: 284002\n",
      "INDEX: 3699 Length: 288703\n",
      "INDEX: 3749 Length: 292649\n",
      "INDEX: 3799 Length: 295819\n",
      "INDEX: 3849 Length: 301751\n",
      "INDEX: 3899 Length: 306082\n",
      "INDEX: 3949 Length: 309745\n",
      "INDEX: 3999 Length: 313411\n",
      "INDEX: 4049 Length: 317713\n",
      "INDEX: 4099 Length: 322921\n",
      "INDEX: 4149 Length: 328426\n",
      "INDEX: 4199 Length: 332231\n",
      "INDEX: 4249 Length: 336086\n",
      "INDEX: 4299 Length: 339491\n",
      "INDEX: 4349 Length: 342867\n",
      "INDEX: 4399 Length: 346780\n",
      "INDEX: 4449 Length: 350614\n",
      "INDEX: 4499 Length: 354422\n",
      "INDEX: 4549 Length: 358686\n",
      "INDEX: 4599 Length: 362255\n",
      "INDEX: 4649 Length: 365883\n",
      "(367062,)\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(sentences):\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    count = 0\n",
    "    print(\"started\")\n",
    "    for j, i in enumerate(sentences):\n",
    "#         if (j + 1) % 1000 == 0:\n",
    "#             count += 1\n",
    "#             x_train_npy = np.array(x_train).reshape(len(x_train), len(V))\n",
    "#             y_train_npy = np.array(y_train).reshape(len(x_train), len(V))\n",
    "#             np.save('x_train' + str(count), x_train_npy)\n",
    "#             np.save('y_train' + str(count), y_train_npy)\n",
    "#             del x_train_npy\n",
    "#             del y_train_npy\n",
    "#             del x_train\n",
    "#             del y_train\n",
    "#             x_train = []\n",
    "#             y_train = []\n",
    "        if (j + 1) % 50 == 0:\n",
    "            print(\"INDEX:\", j, \"Length:\",len(x_train))\n",
    "        word_context = get_target_context(i, window_size)\n",
    "        for word, context in word_context:\n",
    "            input_vec = not_word_to_one_hot(word, len(V))\n",
    "#             input_vec = input_vec.reshape((1,len(V)))\n",
    "            for j in context:\n",
    "                output_vec = not_word_to_one_hot(j, len(V))\n",
    "#                 output_vec = word_to_one_hot(j).reshape(1, len(V))\n",
    "                x_train.append(input_vec)\n",
    "                y_train.append(output_vec)\n",
    "#     if len(x_train) > 0:\n",
    "#         count += 2\n",
    "#         x_train_npy = np.array(x_train).reshape(len(x_train), len(V))\n",
    "#         y_train_npy = np.array(y_train).reshape(len(x_train), len(V))\n",
    "#         np.save('x_train' + str(count), x_train_npy)\n",
    "#         np.save('y_train' + str(count), y_train_npy)\n",
    "#         del x_train_npy\n",
    "#         del y_train_npy\n",
    "#         del x_train\n",
    "#         del y_train\n",
    "#         x_train = []\n",
    "#         y_train = []\n",
    "                \n",
    "#     x_train = np.array(x_train).reshape(len(x_train), len(V))\n",
    "#     y_train = np.array(y_train).reshape(len(x_train), len(V))\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = np.array(y_train)\n",
    "    np.save('x_train_bengali', x_train)\n",
    "    np.save('y_train_bengali', y_train)\n",
    "    return x_train, y_train\n",
    "print(len(sentences))\n",
    "x_train, y_train = create_dataset(sentences)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('x_train2000',x_train)\n",
    "# np.save('y_train2000',y_train)\n",
    "# Total words: 57333\n",
    "# Unique words: 11223\n",
    "# x_train = np.concatenate((np.load('x_train1.npy'), np.load('x_train2.npy'), np.load('x_train3.npy')), axis=0)\n",
    "# y_train = np.concatenate((np.load('y_train1.npy'), np.load('y_train2.npy'), np.load('y_train3.npy')), axis=0)\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "# y_train = np.load('y_train1.npy')\n",
    "# del x_train\n",
    "print(x_train.shape)\n",
    "print(max(x_train))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "367062\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0])\n",
    "batch_size = 1000\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "x_train = data_utils.TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(x_train, batch_size=batch_size, shuffle=True)\n",
    "# del x_train\n",
    "# print(len(V))\n",
    "features = len(V)\n",
    "total_rows = len(y_train)\n",
    "print(total_rows)\n",
    "# del y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckTfK78Ew8wI"
   },
   "source": [
    "# 1.9 Training the model (3 points)\n",
    "\n",
    "As everything is prepared, implement a training loop that performs several passes of the data set through the model. You are free to do this as you please, but your code should:\n",
    "\n",
    "* Load the weights saved in 1.6 at the start of every execution of the code block\n",
    "* Print the accumulated loss at least after every epoch (the accumulate loss should be reset after every epoch)\n",
    "* Define a criterion for the training procedure to terminate if a certain loss value is reached. You can find the threshold by observing the loss for the development set.\n",
    "\n",
    "You can play around with the number of epochs and the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-d75c1c675acc>:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  loss = criterion(y_pred_torch, torch.max(torch.tensor(y_batch), 1)[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 8.256557944028273\n",
      "Epoch: 1 \tLoss: 7.144751563020375\n",
      "Epoch: 2 \tLoss: 6.826389645752699\n",
      "Epoch: 3 \tLoss: 6.659692014041155\n",
      "Epoch: 4 \tLoss: 6.5556360807107845\n",
      "Epoch: 5 \tLoss: 6.473489030547764\n",
      "Epoch: 6 \tLoss: 6.411970576514369\n",
      "Epoch: 7 \tLoss: 6.361793383308079\n",
      "Epoch: 8 \tLoss: 6.319921716399815\n",
      "Epoch: 9 \tLoss: 6.292960072341173\n",
      "Epoch: 10 \tLoss: 6.2668399616428045\n",
      "Epoch: 11 \tLoss: 6.245123685702033\n",
      "Epoch: 12 \tLoss: 6.2249729594458705\n",
      "Epoch: 13 \tLoss: 6.20512618318848\n",
      "Epoch: 14 \tLoss: 6.190873468699663\n",
      "Epoch: 15 \tLoss: 6.178409030903941\n",
      "Epoch: 16 \tLoss: 6.161919405926829\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer and loss\n",
    "\n",
    "net = Word2Vec(len(V), embedding_size)\n",
    "# net.load_state_dict(torch.load('model_param'))\n",
    "net.to(dev)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train():\n",
    "    print(\"Started:\")\n",
    "    loss_vals=  []\n",
    "    for e in range(epochs):\n",
    "        epoch_loss= []\n",
    "        for i, (X_batch, y_batch) in enumerate(train_dl):\n",
    "#             X_batch = X_batch.cpu().detach().numpy()\n",
    "#             print (X_batch)\n",
    "#             if (i + 1) % 100 == 0:\n",
    "#                 print('Batch: ', (i+1))\n",
    "            X_batch = np.eye(len(V))[X_batch]\n",
    "            y_batch = np.eye(len(V))[y_batch]\n",
    "            X_batch = torch.Tensor(X_batch).to(dev)\n",
    "            y_batch = torch.Tensor(y_batch).to(dev)\n",
    "#             print(X_batch)\n",
    "#             input_vec = word_to_one_hot(X_batch, y_train.shape[0])\n",
    "#             input_vec = input_vec\n",
    "#             input_vec = torch.tensor(input_vec)\n",
    "            y_pred_torch = net(X_batch)\n",
    "            loss = criterion(y_pred_torch, torch.max(torch.tensor(y_batch), 1)[1])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            epoch_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "        loss_vals.append(sum(epoch_loss)/len(epoch_loss))\n",
    "        print('Epoch:', e,'\\tLoss:',loss_vals[e])\n",
    "#         break\n",
    "    plt.plot(np.linspace(1, 100, 100).astype(int), loss_vals)\n",
    "\n",
    "train()\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgQkaYstyj0Q"
   },
   "source": [
    "# 1.10 Train on the full dataset (0.5 points)\n",
    "\n",
    "Now, go back to 1.1 and remove the restriction on the number of sentences in your corpus. Then, reexecute code blocks 1.2, 1.3 and 1.6 (or those relevant if you created additional ones). \n",
    "\n",
    "* Then, retrain your model on the complete dataset.\n",
    "\n",
    "* Now, the input weights of the model contain the desired word embeddings! Save them together with the corresponding vocabulary items (Pytorch provides a nice [functionality](https://pytorch.org/tutorials/beginner/saving_loading_models.html) for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x8hQP_bg4_g"
   },
   "outputs": [],
   "source": [
    "torch.save(net, 'model_finalised_bengali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load('model_finalised_bengali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model_param_finalised_bengali')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NNTI_final_project_task_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
