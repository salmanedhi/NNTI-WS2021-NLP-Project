{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch as torch\n",
    "from torch import nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math as mt\n",
    "import time\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "import helper\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"#\"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "url = 'https://raw.githubusercontent.com/salmanedhi/NNTI-WS2021-NLP-Project/main/data/hindi_hatespeech.tsv'\n",
    "\n",
    "data = pd.read_csv(url, sep='\\t')\n",
    "    \n",
    "#data = pd.read_csv('data/hindi_hatespeech.tsv', sep='\\t')\n",
    "data_development = shuffle(data)\n",
    "labels = data_development['task_2']\n",
    "# data_development = data\n",
    "type(data_development['task_1'])\n",
    "\n",
    "print(\"Done\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  4665\n",
      "Total words: 141550\n",
      "Unique words: 19836\n"
     ]
    }
   ],
   "source": [
    "#Stopwords Removal\n",
    "sentences = helper.apply_stopword_removal(data_development)\n",
    "print(\"Number of sentences: \" , len(sentences))\n",
    "\n",
    "#Building Vocabulary\n",
    "V, non_unique = helper.build_vocabulary(sentences)\n",
    "print('Total words:', len(non_unique))\n",
    "print('Unique words:', len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(\n",
      "  (fc1): Linear(in_features=19836, out_features=600, bias=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19836, bias=True)\n",
      ")\n",
      "torch.Size([19837, 600]) torch.Size([600, 19836])\n"
     ]
    }
   ],
   "source": [
    "#Load Word2Vec embeddings module\n",
    "weights1, weights2 = helper.load_word2vec_embeddings('model_param_finalised', device, len(V), 600)\n",
    "print(weights1.shape, weights2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4665, 132)\n",
      "(4665,)\n"
     ]
    }
   ],
   "source": [
    "## create number array of sentences (replace each word with each numeric value)\n",
    "x_data, max_len_curr = helper.sentence_to_numeric_arr(sentences, V)\n",
    "\n",
    "## apply padding\n",
    "padded = np.array(helper.padding(x_data, max_len_curr))\n",
    "\n",
    "print(padded.shape)\n",
    "encoded_labels = [0 if label == \"NONE\" else 1 for label in labels]\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "print(encoded_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#split data into train, valid & test set\n",
    "\n",
    "print(type(padded), type(encoded_labels))\n",
    "batch_size = 64\n",
    "train_loader, valid_loader, test_loader = helper.split_data_train_valid_test(padded, encoded_labels, batch_size)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(19837, 600)\n",
      "  (lstm): LSTM(600, 32, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net, criterion= helper.initialize_SentimentLSTM_model(len(V) + 1, 600, 32, 1, 2, device, weights1)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-615d560f5db7>:23: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200 Training Loss: 0.6911\n",
      "Epoch: 2/200 Step: 100 Training Loss: 0.6563 Validation Loss: 0.6563\n",
      "Epoch: 2/200 Training Loss: 0.6775\n",
      "Epoch: 3/200 Training Loss: 0.6708\n",
      "Epoch: 4/200 Step: 200 Training Loss: 0.6521 Validation Loss: 0.6470\n",
      "Epoch: 4/200 Training Loss: 0.6696\n",
      "Epoch: 5/200 Training Loss: 0.6630\n",
      "Epoch: 6/200 Step: 300 Training Loss: 0.6577 Validation Loss: 0.6626\n",
      "Epoch: 6/200 Training Loss: 0.6587\n",
      "Epoch: 7/200 Step: 400 Training Loss: 0.6461 Validation Loss: 0.6421\n",
      "Epoch: 7/200 Training Loss: 0.6600\n",
      "Epoch: 8/200 Training Loss: 0.7428\n",
      "Epoch: 9/200 Step: 500 Training Loss: 0.6911 Validation Loss: 0.6923\n",
      "Epoch: 9/200 Training Loss: 0.6933\n",
      "Epoch: 10/200 Training Loss: 0.6927\n",
      "Epoch: 11/200 Step: 600 Training Loss: 0.6951 Validation Loss: 0.6981\n",
      "Epoch: 11/200 Training Loss: 0.6929\n",
      "Epoch: 12/200 Training Loss: 0.6914\n",
      "Epoch: 13/200 Step: 700 Training Loss: 0.6917 Validation Loss: 0.6909\n",
      "Epoch: 13/200 Training Loss: 0.6913\n",
      "Epoch: 14/200 Step: 800 Training Loss: 0.6830 Validation Loss: 0.6903\n",
      "Epoch: 14/200 Training Loss: 0.6926\n",
      "Epoch: 15/200 Training Loss: 0.6911\n",
      "Epoch: 16/200 Step: 900 Training Loss: 0.6966 Validation Loss: 0.6914\n",
      "Epoch: 16/200 Training Loss: 0.6900\n",
      "Epoch: 17/200 Training Loss: 0.6881\n",
      "Epoch: 18/200 Step: 1000 Training Loss: 0.6845 Validation Loss: 0.6895\n",
      "Epoch: 18/200 Training Loss: 0.6811\n",
      "Epoch: 19/200 Step: 1100 Training Loss: 0.6836 Validation Loss: 0.6794\n",
      "Epoch: 19/200 Training Loss: 0.6793\n",
      "Epoch: 20/200 Training Loss: 0.6729\n",
      "Epoch: 21/200 Step: 1200 Training Loss: 0.6853 Validation Loss: 0.6905\n",
      "Epoch: 21/200 Training Loss: 0.6684\n",
      "Epoch: 22/200 Training Loss: 0.6579\n",
      "Epoch: 23/200 Step: 1300 Training Loss: 0.5995 Validation Loss: 0.5896\n",
      "Epoch: 23/200 Training Loss: 0.6490\n",
      "Epoch: 24/200 Training Loss: 0.6343\n",
      "Epoch: 25/200 Step: 1400 Training Loss: 0.6663 Validation Loss: 0.6924\n",
      "Epoch: 25/200 Training Loss: 0.6321\n",
      "Epoch: 26/200 Step: 1500 Training Loss: 0.5763 Validation Loss: 0.5653\n",
      "Epoch: 26/200 Training Loss: 0.6083\n",
      "Epoch: 27/200 Training Loss: 0.6056\n",
      "Epoch: 28/200 Step: 1600 Training Loss: 0.5530 Validation Loss: 0.5474\n",
      "Epoch: 28/200 Training Loss: 0.5821\n",
      "Epoch: 29/200 Training Loss: 0.5732\n",
      "Epoch: 30/200 Step: 1700 Training Loss: 0.5805 Validation Loss: 0.5775\n",
      "Epoch: 30/200 Training Loss: 0.5576\n",
      "Epoch: 31/200 Training Loss: 0.5493\n",
      "Epoch: 32/200 Step: 1800 Training Loss: 0.6100 Validation Loss: 0.5649\n",
      "Epoch: 32/200 Training Loss: 0.5469\n",
      "Epoch: 33/200 Step: 1900 Training Loss: 0.5725 Validation Loss: 0.5246\n",
      "Epoch: 33/200 Training Loss: 0.5273\n",
      "Epoch: 34/200 Training Loss: 0.5198\n",
      "Epoch: 35/200 Step: 2000 Training Loss: 0.5596 Validation Loss: 0.5494\n",
      "Epoch: 35/200 Training Loss: 0.5065\n",
      "Epoch: 36/200 Training Loss: 0.5044\n",
      "Epoch: 37/200 Step: 2100 Training Loss: 0.4540 Validation Loss: 0.4091\n",
      "Epoch: 37/200 Training Loss: 0.4846\n",
      "Epoch: 38/200 Step: 2200 Training Loss: 0.5329 Validation Loss: 0.5054\n",
      "Epoch: 38/200 Training Loss: 0.4685\n",
      "Epoch: 39/200 Training Loss: 0.4720\n",
      "Epoch: 40/200 Step: 2300 Training Loss: 0.4948 Validation Loss: 0.5140\n",
      "Epoch: 40/200 Training Loss: 0.4665\n",
      "Epoch: 41/200 Training Loss: 0.4540\n",
      "Epoch: 42/200 Step: 2400 Training Loss: 0.3873 Validation Loss: 0.8726\n",
      "Epoch: 42/200 Training Loss: 0.4730\n",
      "Epoch: 43/200 Training Loss: 0.4572\n",
      "Epoch: 44/200 Step: 2500 Training Loss: 0.5014 Validation Loss: 0.4832\n",
      "Epoch: 44/200 Training Loss: 0.4362\n",
      "Epoch: 45/200 Step: 2600 Training Loss: 0.4418 Validation Loss: 0.4323\n",
      "Epoch: 45/200 Training Loss: 0.4201\n",
      "Epoch: 46/200 Training Loss: 0.4276\n",
      "Epoch: 47/200 Step: 2700 Training Loss: 0.5261 Validation Loss: 0.5021\n",
      "Epoch: 47/200 Training Loss: 0.4144\n",
      "Epoch: 48/200 Training Loss: 0.4026\n",
      "Epoch: 49/200 Step: 2800 Training Loss: 0.3938 Validation Loss: 0.3630\n",
      "Epoch: 49/200 Training Loss: 0.4036\n",
      "Epoch: 50/200 Step: 2900 Training Loss: 0.5050 Validation Loss: 0.4851\n",
      "Epoch: 50/200 Training Loss: 0.4061\n",
      "Epoch: 51/200 Training Loss: 0.4002\n",
      "Epoch: 52/200 Step: 3000 Training Loss: 0.4862 Validation Loss: 0.4822\n",
      "Epoch: 52/200 Training Loss: 0.3998\n",
      "Epoch: 53/200 Training Loss: 0.4261\n",
      "Epoch: 54/200 Step: 3100 Training Loss: 0.3699 Validation Loss: 0.3475\n",
      "Epoch: 54/200 Training Loss: 0.3780\n",
      "Epoch: 55/200 Training Loss: 0.3695\n",
      "Epoch: 56/200 Step: 3200 Training Loss: 0.3563 Validation Loss: 0.3526\n",
      "Epoch: 56/200 Training Loss: 0.3639\n",
      "Epoch: 57/200 Step: 3300 Training Loss: 0.3144 Validation Loss: 0.2996\n",
      "Epoch: 57/200 Training Loss: 0.3527\n",
      "Epoch: 58/200 Training Loss: 0.3499\n",
      "Epoch: 59/200 Step: 3400 Training Loss: 0.3204 Validation Loss: 0.2601\n",
      "Epoch: 59/200 Training Loss: 0.3407\n",
      "Epoch: 60/200 Training Loss: 0.3524\n",
      "Epoch: 61/200 Step: 3500 Training Loss: 0.2684 Validation Loss: 0.2429\n",
      "Epoch: 61/200 Training Loss: 0.3310\n",
      "Epoch: 62/200 Training Loss: 0.3254\n",
      "Epoch: 63/200 Step: 3600 Training Loss: 0.1641 Validation Loss: 0.1481\n",
      "Epoch: 63/200 Training Loss: 0.3159\n",
      "Epoch: 64/200 Step: 3700 Training Loss: 0.2523 Validation Loss: 0.2530\n",
      "Epoch: 64/200 Training Loss: 0.3299\n",
      "Epoch: 65/200 Training Loss: 0.3092\n",
      "Epoch: 66/200 Step: 3800 Training Loss: 0.3926 Validation Loss: 0.4181\n",
      "Epoch: 66/200 Training Loss: 0.3204\n",
      "Epoch: 67/200 Training Loss: 0.3221\n",
      "Epoch: 68/200 Step: 3900 Training Loss: 0.3137 Validation Loss: 0.2387\n",
      "Epoch: 68/200 Training Loss: 0.3091\n",
      "Epoch: 69/200 Step: 4000 Training Loss: 0.2666 Validation Loss: 0.2140\n",
      "Epoch: 69/200 Training Loss: 0.2973\n",
      "Epoch: 70/200 Training Loss: 0.2853\n",
      "Epoch: 71/200 Step: 4100 Training Loss: 0.2173 Validation Loss: 0.2086\n",
      "Epoch: 71/200 Training Loss: 0.2804\n",
      "Epoch: 72/200 Training Loss: 0.2745\n",
      "Epoch: 73/200 Step: 4200 Training Loss: 0.3182 Validation Loss: 0.2913\n",
      "Epoch: 73/200 Training Loss: 0.2809\n",
      "Epoch: 74/200 Training Loss: 0.2587\n",
      "Epoch: 75/200 Step: 4300 Training Loss: 0.2548 Validation Loss: 0.2888\n",
      "Epoch: 75/200 Training Loss: 0.2527\n",
      "Epoch: 76/200 Step: 4400 Training Loss: 0.2489 Validation Loss: 0.2742\n",
      "Epoch: 76/200 Training Loss: 0.2476\n",
      "Epoch: 77/200 Training Loss: 0.2455\n",
      "Epoch: 78/200 Step: 4500 Training Loss: 0.1208 Validation Loss: 0.1129\n",
      "Epoch: 78/200 Training Loss: 0.2571\n",
      "Epoch: 79/200 Training Loss: 0.2453\n",
      "Epoch: 80/200 Step: 4600 Training Loss: 0.1088 Validation Loss: 0.0730\n",
      "Epoch: 80/200 Training Loss: 0.2547\n",
      "Epoch: 81/200 Training Loss: 0.2179\n",
      "Epoch: 82/200 Step: 4700 Training Loss: 0.1269 Validation Loss: 0.0893\n",
      "Epoch: 82/200 Training Loss: 0.2210\n",
      "Epoch: 83/200 Step: 4800 Training Loss: 0.1550 Validation Loss: 0.1298\n",
      "Epoch: 83/200 Training Loss: 0.2240\n",
      "Epoch: 84/200 Training Loss: 0.2333\n",
      "Epoch: 85/200 Step: 4900 Training Loss: 0.1453 Validation Loss: 0.1172\n",
      "Epoch: 85/200 Training Loss: 0.2201\n",
      "Epoch: 86/200 Training Loss: 0.2193\n",
      "Epoch: 87/200 Step: 5000 Training Loss: 0.3004 Validation Loss: 0.2847\n",
      "Epoch: 87/200 Training Loss: 0.2014\n",
      "Epoch: 88/200 Step: 5100 Training Loss: 0.2534 Validation Loss: 0.2016\n",
      "Epoch: 88/200 Training Loss: 0.1978\n",
      "Epoch: 89/200 Training Loss: 0.2087\n",
      "Epoch: 90/200 Step: 5200 Training Loss: 0.1827 Validation Loss: 0.1243\n",
      "Epoch: 90/200 Training Loss: 0.1955\n",
      "Epoch: 91/200 Training Loss: 0.2079\n",
      "Epoch: 92/200 Step: 5300 Training Loss: 0.1489 Validation Loss: 0.1279\n",
      "Epoch: 92/200 Training Loss: 0.2029\n",
      "Epoch: 93/200 Training Loss: 0.1734\n",
      "Epoch: 94/200 Step: 5400 Training Loss: 0.1805 Validation Loss: 0.1391\n",
      "Epoch: 94/200 Training Loss: 0.2056\n",
      "Epoch: 95/200 Step: 5500 Training Loss: 0.1134 Validation Loss: 0.0928\n",
      "Epoch: 95/200 Training Loss: 0.1793\n",
      "Epoch: 96/200 Training Loss: 0.1762\n",
      "Epoch: 97/200 Step: 5600 Training Loss: 0.1647 Validation Loss: 0.0953\n",
      "Epoch: 97/200 Training Loss: 0.1776\n",
      "Epoch: 98/200 Training Loss: 0.1832\n",
      "Epoch: 99/200 Step: 5700 Training Loss: 0.2634 Validation Loss: 0.1795\n",
      "Epoch: 99/200 Training Loss: 0.1854\n",
      "Epoch: 100/200 Step: 5800 Training Loss: 0.2373 Validation Loss: 0.1827\n",
      "Epoch: 100/200 Training Loss: 0.1895\n",
      "Epoch: 101/200 Training Loss: 0.1691\n",
      "Epoch: 102/200 Step: 5900 Training Loss: 0.3682 Validation Loss: 0.3091\n",
      "Epoch: 102/200 Training Loss: 0.1982\n",
      "Epoch: 103/200 Training Loss: 0.1582\n",
      "Epoch: 104/200 Step: 6000 Training Loss: 0.2425 Validation Loss: 0.1664\n",
      "Epoch: 104/200 Training Loss: 0.1869\n",
      "Epoch: 105/200 Training Loss: 0.1498\n",
      "Epoch: 106/200 Step: 6100 Training Loss: 0.1868 Validation Loss: 0.1634\n",
      "Epoch: 106/200 Training Loss: 0.1430\n",
      "Epoch: 107/200 Step: 6200 Training Loss: 0.1038 Validation Loss: 0.0942\n",
      "Epoch: 107/200 Training Loss: 0.1513\n",
      "Epoch: 108/200 Training Loss: 0.1666\n",
      "Epoch: 109/200 Step: 6300 Training Loss: 0.2793 Validation Loss: 0.2869\n",
      "Epoch: 109/200 Training Loss: 0.1458\n",
      "Epoch: 110/200 Training Loss: 0.1341\n",
      "Epoch: 111/200 Step: 6400 Training Loss: 0.1221 Validation Loss: 0.1237\n",
      "Epoch: 111/200 Training Loss: 0.1265\n",
      "Epoch: 112/200 Training Loss: 0.1486\n",
      "Epoch: 113/200 Step: 6500 Training Loss: 0.1114 Validation Loss: 0.1055\n",
      "Epoch: 113/200 Training Loss: 0.1542\n",
      "Epoch: 114/200 Step: 6600 Training Loss: 0.0855 Validation Loss: 0.0390\n",
      "Epoch: 114/200 Training Loss: 0.1228\n",
      "Epoch: 115/200 Training Loss: 0.1465\n",
      "Epoch: 116/200 Step: 6700 Training Loss: 0.0519 Validation Loss: 0.0402\n",
      "Epoch: 116/200 Training Loss: 0.1297\n",
      "Epoch: 117/200 Training Loss: 0.1210\n",
      "Epoch: 118/200 Step: 6800 Training Loss: 0.0384 Validation Loss: 0.0336\n",
      "Epoch: 118/200 Training Loss: 0.1188\n",
      "Epoch: 119/200 Step: 6900 Training Loss: 0.0593 Validation Loss: 0.0615\n",
      "Epoch: 119/200 Training Loss: 0.1124\n",
      "Epoch: 120/200 Training Loss: 0.1188\n",
      "Epoch: 121/200 Step: 7000 Training Loss: 0.0485 Validation Loss: 0.0631\n",
      "Epoch: 121/200 Training Loss: 0.1069\n",
      "Epoch: 122/200 Training Loss: 0.1179\n",
      "Epoch: 123/200 Step: 7100 Training Loss: 0.1362 Validation Loss: 0.0977\n",
      "Epoch: 123/200 Training Loss: 0.1234\n",
      "Epoch: 124/200 Training Loss: 0.1329\n",
      "Epoch: 125/200 Step: 7200 Training Loss: 0.1573 Validation Loss: 0.1061\n",
      "Epoch: 125/200 Training Loss: 0.1324\n",
      "Epoch: 126/200 Step: 7300 Training Loss: 0.1144 Validation Loss: 0.0951\n",
      "Epoch: 126/200 Training Loss: 0.1197\n",
      "Epoch: 127/200 Training Loss: 0.1149\n",
      "Epoch: 128/200 Step: 7400 Training Loss: 0.1157 Validation Loss: 0.0497\n",
      "Epoch: 128/200 Training Loss: 0.1068\n",
      "Epoch: 129/200 Training Loss: 0.0995\n",
      "Epoch: 130/200 Step: 7500 Training Loss: 0.0778 Validation Loss: 0.0436\n",
      "Epoch: 130/200 Training Loss: 0.0935\n",
      "Epoch: 131/200 Training Loss: 0.1016\n",
      "Epoch: 132/200 Step: 7600 Training Loss: 0.1204 Validation Loss: 0.0977\n",
      "Epoch: 132/200 Training Loss: 0.1254\n",
      "Epoch: 133/200 Step: 7700 Training Loss: 0.0938 Validation Loss: 0.0501\n",
      "Epoch: 133/200 Training Loss: 0.1127\n",
      "Epoch: 134/200 Training Loss: 0.1212\n",
      "Epoch: 135/200 Step: 7800 Training Loss: 0.0220 Validation Loss: 0.0131\n",
      "Epoch: 135/200 Training Loss: 0.1080\n",
      "Epoch: 136/200 Training Loss: 0.0937\n",
      "Epoch: 137/200 Step: 7900 Training Loss: 0.1069 Validation Loss: 0.0463\n",
      "Epoch: 137/200 Training Loss: 0.0933\n",
      "Epoch: 138/200 Step: 8000 Training Loss: 0.1977 Validation Loss: 0.1649\n",
      "Epoch: 138/200 Training Loss: 0.0879\n",
      "Epoch: 139/200 Training Loss: 0.0929\n",
      "Epoch: 140/200 Step: 8100 Training Loss: 0.2988 Validation Loss: 0.2671\n",
      "Epoch: 140/200 Training Loss: 0.0909\n",
      "Epoch: 141/200 Training Loss: 0.0895\n",
      "Epoch: 142/200 Step: 8200 Training Loss: 0.1147 Validation Loss: 0.0769\n",
      "Epoch: 142/200 Training Loss: 0.1047\n",
      "Epoch: 143/200 Training Loss: 0.0866\n",
      "Epoch: 144/200 Step: 8300 Training Loss: 0.1338 Validation Loss: 0.1182\n",
      "Epoch: 144/200 Training Loss: 0.0836\n",
      "Epoch: 145/200 Step: 8400 Training Loss: 0.0390 Validation Loss: 0.0232\n",
      "Epoch: 145/200 Training Loss: 0.0867\n",
      "Epoch: 146/200 Training Loss: 0.1086\n",
      "Epoch: 147/200 Step: 8500 Training Loss: 0.0513 Validation Loss: 0.0191\n",
      "Epoch: 147/200 Training Loss: 0.0781\n",
      "Epoch: 148/200 Training Loss: 0.0863\n",
      "Epoch: 149/200 Step: 8600 Training Loss: 0.1287 Validation Loss: 0.0769\n",
      "Epoch: 149/200 Training Loss: 0.0996\n",
      "Epoch: 150/200 Step: 8700 Training Loss: 0.0976 Validation Loss: 0.0933\n",
      "Epoch: 150/200 Training Loss: 0.0844\n",
      "Epoch: 151/200 Training Loss: 0.0985\n",
      "Epoch: 152/200 Step: 8800 Training Loss: 0.0427 Validation Loss: 0.0150\n",
      "Epoch: 152/200 Training Loss: 0.0904\n",
      "Epoch: 153/200 Training Loss: 0.0866\n",
      "Epoch: 154/200 Step: 8900 Training Loss: 0.0339 Validation Loss: 0.0102\n",
      "Epoch: 154/200 Training Loss: 0.0865\n",
      "Epoch: 155/200 Training Loss: 0.0689\n",
      "Epoch: 156/200 Step: 9000 Training Loss: 0.1227 Validation Loss: 0.0989\n",
      "Epoch: 156/200 Training Loss: 0.0790\n",
      "Epoch: 157/200 Step: 9100 Training Loss: 0.1337 Validation Loss: 0.0998\n",
      "Epoch: 157/200 Training Loss: 0.0834\n",
      "Epoch: 158/200 Training Loss: 0.0814\n",
      "Epoch: 159/200 Step: 9200 Training Loss: 0.0154 Validation Loss: 0.0103\n",
      "Epoch: 159/200 Training Loss: 0.0731\n",
      "Epoch: 160/200 Training Loss: 0.0749\n",
      "Epoch: 161/200 Step: 9300 Training Loss: 0.0370 Validation Loss: 0.0239\n",
      "Epoch: 161/200 Training Loss: 0.0712\n",
      "Epoch: 162/200 Training Loss: 0.0643\n",
      "Epoch: 163/200 Step: 9400 Training Loss: 0.1149 Validation Loss: 0.1244\n",
      "Epoch: 163/200 Training Loss: 0.0731\n",
      "Epoch: 164/200 Step: 9500 Training Loss: 0.2642 Validation Loss: 0.1532\n",
      "Epoch: 164/200 Training Loss: 0.0641\n",
      "Epoch: 165/200 Training Loss: 0.0739\n",
      "Epoch: 166/200 Step: 9600 Training Loss: 0.1230 Validation Loss: 0.1015\n",
      "Epoch: 166/200 Training Loss: 0.0667\n",
      "Epoch: 167/200 Training Loss: 0.0601\n",
      "Epoch: 168/200 Step: 9700 Training Loss: 0.1228 Validation Loss: 0.0552\n",
      "Epoch: 168/200 Training Loss: 0.0667\n",
      "Epoch: 169/200 Step: 9800 Training Loss: 0.0327 Validation Loss: 0.0039\n",
      "Epoch: 169/200 Training Loss: 0.0782\n",
      "Epoch: 170/200 Training Loss: 0.0625\n",
      "Epoch: 171/200 Step: 9900 Training Loss: 0.0428 Validation Loss: 0.0328\n",
      "Epoch: 171/200 Training Loss: 0.0703\n",
      "Epoch: 172/200 Training Loss: 0.0812\n",
      "Epoch: 173/200 Step: 10000 Training Loss: 0.0552 Validation Loss: 0.0526\n",
      "Epoch: 173/200 Training Loss: 0.0794\n",
      "Epoch: 174/200 Training Loss: 0.0640\n",
      "Epoch: 175/200 Step: 10100 Training Loss: 0.0067 Validation Loss: 0.0098\n",
      "Epoch: 175/200 Training Loss: 0.0537\n",
      "Epoch: 176/200 Step: 10200 Training Loss: 0.1408 Validation Loss: 0.0933\n",
      "Epoch: 176/200 Training Loss: 0.0798\n",
      "Epoch: 177/200 Training Loss: 0.0594\n",
      "Epoch: 178/200 Step: 10300 Training Loss: 0.1251 Validation Loss: 0.0846\n",
      "Epoch: 178/200 Training Loss: 0.0688\n",
      "Epoch: 179/200 Training Loss: 0.0598\n",
      "Epoch: 180/200 Step: 10400 Training Loss: 0.1296 Validation Loss: 0.0468\n",
      "Epoch: 180/200 Training Loss: 0.0541\n",
      "Epoch: 181/200 Training Loss: 0.0589\n",
      "Epoch: 182/200 Step: 10500 Training Loss: 0.0097 Validation Loss: 0.0161\n",
      "Epoch: 182/200 Training Loss: 0.0775\n",
      "Epoch: 183/200 Step: 10600 Training Loss: 0.1092 Validation Loss: 0.0947\n",
      "Epoch: 183/200 Training Loss: 0.0552\n",
      "Epoch: 184/200 Training Loss: 0.0614\n",
      "Epoch: 185/200 Step: 10700 Training Loss: 0.1157 Validation Loss: 0.0035\n",
      "Epoch: 185/200 Training Loss: 0.0538\n",
      "Epoch: 186/200 Training Loss: 0.0715\n",
      "Epoch: 187/200 Step: 10800 Training Loss: 0.0264 Validation Loss: 0.0092\n",
      "Epoch: 187/200 Training Loss: 0.0540\n",
      "Epoch: 188/200 Step: 10900 Training Loss: 0.0249 Validation Loss: 0.0165\n",
      "Epoch: 188/200 Training Loss: 0.0775\n",
      "Epoch: 189/200 Training Loss: 0.0687\n",
      "Epoch: 190/200 Step: 11000 Training Loss: 0.0366 Validation Loss: 0.0128\n",
      "Epoch: 190/200 Training Loss: 0.0648\n",
      "Epoch: 191/200 Training Loss: 0.0497\n",
      "Epoch: 192/200 Step: 11100 Training Loss: 0.0721 Validation Loss: 0.0758\n",
      "Epoch: 192/200 Training Loss: 0.0468\n",
      "Epoch: 193/200 Training Loss: 0.0725\n",
      "Epoch: 194/200 Step: 11200 Training Loss: 0.1792 Validation Loss: 0.1656\n",
      "Epoch: 194/200 Training Loss: 0.0540\n",
      "Epoch: 195/200 Step: 11300 Training Loss: 0.0537 Validation Loss: 0.0290\n",
      "Epoch: 195/200 Training Loss: 0.0483\n",
      "Epoch: 196/200 Training Loss: 0.0407\n",
      "Epoch: 197/200 Step: 11400 Training Loss: 0.0055 Validation Loss: 0.0041\n",
      "Epoch: 197/200 Training Loss: 0.0520\n",
      "Epoch: 198/200 Training Loss: 0.0426\n",
      "Epoch: 199/200 Step: 11500 Training Loss: 0.0825 Validation Loss: 0.0565\n",
      "Epoch: 199/200 Training Loss: 0.0472\n",
      "Epoch: 200/200 Step: 11600 Training Loss: 0.1879 Validation Loss: 0.1839\n",
      "Epoch: 200/200 Training Loss: 0.0500\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001, amsgrad=True)\n",
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 200  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "\n",
    "training_loss_epoches = []\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size, device)\n",
    "    training_loss = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         print(i, inputs.shape, labels.shape)\n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs, batch_size)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        training_loss.append(loss.item())\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            v_h = net.init_hidden(batch_size, device)\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "                v_output, v_h = net(v_inputs, batch_size)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()\n",
    "    training_loss_epoches.append(np.mean(training_loss))\n",
    "    print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "          \"Training Loss: {:.4f}\".format(np.mean(training_loss)))\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-6.6557e-03,  7.0117e-03, -2.8231e-03,  ...,  7.2872e-03,\n",
      "          3.9233e-04, -3.9099e-03],\n",
      "        [ 2.3458e-03, -3.2887e-03, -3.0864e-03,  ...,  5.5859e-03,\n",
      "          7.9610e-03, -1.1799e-04],\n",
      "        ...,\n",
      "        [-1.5140e-03, -3.0222e-03, -5.4241e-03,  ...,  1.3369e-03,\n",
      "         -5.6326e-04,  7.2260e-03],\n",
      "        [-1.2677e-03, -3.2872e-03, -2.2281e-03,  ...,  3.7003e-03,\n",
      "         -3.3115e-03, -3.5441e-03],\n",
      "        [ 3.2113e-03,  1.9682e-05,  1.4377e-03,  ..., -2.3011e-03,\n",
      "         -3.5223e-03, -1.9228e-03]], device='cuda:0')\n",
      "tensor([[ 0.2149, -0.0815, -0.2503,  ...,  0.1681, -0.2624,  0.5247],\n",
      "        [ 0.0918, -0.3138, -0.0530,  ...,  0.4256, -0.3333, -0.0532],\n",
      "        [-0.0268, -0.2088,  0.2165,  ..., -0.1199, -0.1667,  0.0583],\n",
      "        ...,\n",
      "        [-0.1521, -0.4117,  0.0564,  ...,  0.0353,  0.1782,  0.0133],\n",
      "        [ 0.4272, -0.3766, -0.1141,  ...,  0.1791, -0.1743,  0.2060],\n",
      "        [ 0.1766, -0.1489, -0.1596,  ...,  0.4381,  0.0498,  0.3427]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.3096, -0.1392, -0.1442,  ..., -0.0481,  0.0440, -0.3625],\n",
      "        [ 0.4219,  0.1076, -0.2134,  ..., -0.3176,  0.0223, -0.1716],\n",
      "        [ 0.4161, -0.1976,  0.0108,  ...,  0.0644,  0.0634,  0.1388],\n",
      "        ...,\n",
      "        [ 0.0978,  0.0228,  0.0246,  ...,  0.0231,  0.1796,  0.1840],\n",
      "        [-0.0384, -0.1760,  0.0630,  ...,  0.0600, -0.1366, -0.0627],\n",
      "        [-0.2199,  0.1576, -0.4947,  ..., -0.2672,  0.3707, -0.1955]],\n",
      "       device='cuda:0')\n",
      "tensor([ 2.8049e-01,  1.5539e-01,  4.9576e-02,  1.3663e-01,  1.9448e-01,\n",
      "         1.1231e-03, -3.4551e-02,  1.4305e-01,  3.0724e-01,  1.4258e-01,\n",
      "         7.8118e-02,  1.6138e-01,  1.7411e-01,  1.2925e-01, -5.3327e-02,\n",
      "         2.2461e-02,  4.0513e-01, -8.5798e-02,  2.8027e-01, -2.6466e-02,\n",
      "         5.5800e-01,  4.0425e-02, -8.6988e-03,  2.7346e-01,  3.3595e-01,\n",
      "         2.2991e-01,  1.4636e-01, -3.4070e-02,  8.2397e-01, -3.2779e-02,\n",
      "         3.2743e-01,  1.2621e-01,  7.4277e-02,  1.0059e-01,  1.9603e-01,\n",
      "        -9.4562e-02, -6.5193e-02, -4.8829e-02,  2.0273e-01,  1.5740e-02,\n",
      "         1.0085e-01,  7.4452e-02,  1.4120e-01, -6.0725e-02, -8.2324e-02,\n",
      "        -9.3410e-02, -1.5889e-01, -8.8081e-02,  1.8374e-01, -1.3441e-01,\n",
      "         1.7103e-01,  1.4076e-01, -9.5418e-02,  1.2303e-01,  4.9770e-02,\n",
      "         1.5602e-01,  2.4658e-01, -3.7145e-02,  1.2415e-01,  6.9390e-02,\n",
      "        -2.5800e-02,  6.6813e-02, -7.2017e-03, -6.2960e-02,  1.8214e-02,\n",
      "        -1.1442e-02, -1.0313e-01, -1.1415e-01, -5.1611e-02,  2.4833e-02,\n",
      "        -3.7096e-02, -6.3348e-03, -1.1489e-01, -4.1995e-02, -8.6895e-02,\n",
      "         1.1746e-02,  6.4492e-02,  1.2891e-01,  3.4659e-02, -9.2725e-02,\n",
      "         1.0823e-01,  1.2805e-01,  3.8490e-02, -9.0684e-02, -4.0244e-02,\n",
      "        -1.4270e-01,  2.2836e-02, -6.7067e-02,  1.0910e-01,  6.0825e-02,\n",
      "        -2.3554e-02,  8.0499e-02, -6.2919e-04, -4.3504e-03, -6.9989e-02,\n",
      "         2.8469e-02,  6.1446e-02,  2.2941e-01,  1.7544e-01,  3.0706e-02,\n",
      "         1.2599e-01,  1.3677e-01,  1.0218e-01,  2.0710e-01,  9.5051e-03,\n",
      "         1.4207e-01,  9.7311e-02,  9.8906e-02,  2.9706e-01,  6.9284e-02,\n",
      "        -6.2484e-04,  8.6516e-02,  6.9055e-01, -2.0371e-01,  1.7671e-01,\n",
      "         1.0914e-01,  6.5561e-01, -4.9667e-02,  8.5366e-02,  2.9925e-01,\n",
      "         1.9950e-01, -1.0526e-02, -5.8173e-03,  1.2575e-01,  5.5803e-01,\n",
      "         2.3722e-01,  3.2204e-01,  1.7990e-01], device='cuda:0')\n",
      "tensor([ 0.0899,  0.1562,  0.2272,  0.0620, -0.0248,  0.1409, -0.0107,  0.1537,\n",
      "         0.3474,  0.1121,  0.2777,  0.1189,  0.1919, -0.0692,  0.0560,  0.0891,\n",
      "         0.3498, -0.0739,  0.2708,  0.2292,  0.7131,  0.2417,  0.2367,  0.0308,\n",
      "         0.2492,  0.1629, -0.0442,  0.0920,  0.7484,  0.2373,  0.2901,  0.0222,\n",
      "         0.0042,  0.1804,  0.0415,  0.1599,  0.2476,  0.0234, -0.0592,  0.1075,\n",
      "        -0.0577, -0.0958,  0.1254,  0.1974,  0.0881,  0.1206,  0.0659,  0.2310,\n",
      "         0.0457,  0.0652,  0.1427,  0.1104, -0.1025, -0.1752, -0.0865,  0.2724,\n",
      "         0.0825,  0.1346, -0.0667,  0.1750,  0.0241, -0.0598,  0.2367,  0.1398,\n",
      "        -0.0092,  0.0174, -0.0045, -0.1489, -0.0613,  0.0688, -0.0514,  0.0302,\n",
      "         0.1535,  0.0581,  0.0788, -0.0367, -0.0837, -0.1355, -0.0618,  0.1554,\n",
      "        -0.1110,  0.1125,  0.0150,  0.0481, -0.0494,  0.1060,  0.0871, -0.0593,\n",
      "         0.0328, -0.0496,  0.1428,  0.0208, -0.0905, -0.1079,  0.0975, -0.1383,\n",
      "         0.1623,  0.1214,  0.0157,  0.0535,  0.1795,  0.2162, -0.0296, -0.0966,\n",
      "         0.1972,  0.0997,  0.2252,  0.1479,  0.2983,  0.0651, -0.0155,  0.1864,\n",
      "         0.7148,  0.0010,  0.0803, -0.0251,  0.8269, -0.0417,  0.0030,  0.2240,\n",
      "         0.3149,  0.0964,  0.0344,  0.1082,  0.6748, -0.0686,  0.3171,  0.0538],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.1348, -0.0318,  0.1624,  ..., -0.1285, -0.0417, -0.0390],\n",
      "        [-0.0491,  0.0452,  0.0544,  ..., -0.0842, -0.0459, -0.0820],\n",
      "        [ 0.0309,  0.0245, -0.1580,  ..., -0.0266, -0.0920,  0.2161],\n",
      "        ...,\n",
      "        [ 0.2311, -0.0815,  0.1354,  ..., -0.0988, -0.0558,  0.3044],\n",
      "        [ 0.0090, -0.3406,  0.0504,  ..., -0.0634,  0.4507, -0.2114],\n",
      "        [-0.0855, -0.2461,  0.2654,  ...,  0.1277, -0.1481, -0.0635]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0669, -0.0932,  0.0367,  ...,  0.0176,  0.7299,  0.2535],\n",
      "        [-0.0421,  0.1131,  0.0906,  ...,  0.2799,  0.6600,  0.0233],\n",
      "        [-0.1705, -0.1651, -0.1010,  ...,  0.1704,  0.5484,  0.2893],\n",
      "        ...,\n",
      "        [-0.1946, -0.1181,  0.0116,  ...,  0.1063,  0.7596,  0.1570],\n",
      "        [ 0.1981,  0.2846,  0.0077,  ..., -0.1300,  0.0387,  0.3801],\n",
      "        [ 0.0660, -0.1521, -0.0605,  ..., -0.2593,  0.5291,  0.3365]],\n",
      "       device='cuda:0')\n",
      "tensor([ 2.9303e-01,  4.6237e-01,  4.6415e-01,  3.5042e-01,  1.3105e-01,\n",
      "         4.1878e-01,  4.0792e-01,  1.6201e-01,  1.4115e-01,  1.7096e-01,\n",
      "         2.9764e-01,  1.1163e-01,  3.7869e-01,  5.8453e-01,  4.2491e-01,\n",
      "         3.4269e-01,  3.9812e-01,  4.5064e-01,  2.9079e-01,  4.7572e-01,\n",
      "        -1.2433e-02,  3.0197e-01,  6.6933e-02,  3.3990e-01,  4.2253e-01,\n",
      "         1.1152e-01,  2.2013e-01,  3.7068e-01,  3.0636e-01,  3.4160e-01,\n",
      "         3.5191e-01,  4.7028e-01, -2.6789e-01,  1.0035e-01, -5.5355e-02,\n",
      "        -2.5975e-01,  1.5822e-01, -1.8446e-01,  1.8841e-01,  3.3315e-02,\n",
      "         3.2855e-02,  9.0067e-03,  2.3703e-02, -2.5572e-02, -8.2109e-02,\n",
      "        -1.4088e-01, -6.6210e-02, -6.0294e-02, -1.2085e-01,  7.3188e-02,\n",
      "         5.8824e-02, -9.6769e-02,  5.2675e-02, -2.7976e-02,  1.0417e-01,\n",
      "        -1.2206e-01, -2.3833e-02,  1.8767e-01, -3.1639e-02,  2.6222e-02,\n",
      "         2.4890e-01,  8.8052e-02,  4.2284e-01,  2.2436e-02, -2.2016e-02,\n",
      "         6.1543e-02,  1.2929e-01, -1.4848e-01, -7.7568e-02,  1.4745e-01,\n",
      "         1.8034e-01,  1.5944e-01, -8.0800e-02,  7.1218e-02, -1.4357e-01,\n",
      "        -4.2107e-02, -1.2672e-01, -9.4050e-02, -7.0177e-02,  1.1533e-01,\n",
      "         1.5335e-01,  7.6988e-02,  5.0481e-02,  5.4535e-02, -1.1538e-01,\n",
      "        -1.3007e-01, -9.8011e-02,  4.4977e-02,  5.3251e-02, -1.0391e-01,\n",
      "         2.0780e-02,  5.5893e-02, -1.5282e-01, -1.2964e-01,  3.8829e-02,\n",
      "         7.0312e-02,  2.4104e-01,  3.8800e-01,  3.3407e-01,  4.7458e-01,\n",
      "         3.4414e-03,  3.9762e-01,  1.9699e-01,  2.2895e-01,  2.4217e-01,\n",
      "         7.9330e-02,  1.3105e-01,  6.1272e-02,  4.0831e-01,  1.3332e-01,\n",
      "         2.5793e-01,  2.5068e-01,  4.8361e-01,  4.8937e-01,  6.5490e-05,\n",
      "         8.8578e-02,  1.0350e-01,  3.6253e-01,  2.0490e-01,  4.2434e-01,\n",
      "         3.4990e-01,  9.6914e-02,  3.7610e-01,  4.4561e-01,  1.1194e-01,\n",
      "         2.4070e-01,  4.4536e-01,  3.0084e-01], device='cuda:0')\n",
      "tensor([ 4.6929e-01,  3.7607e-01,  3.6652e-01,  6.3744e-01,  2.0028e-01,\n",
      "         2.7035e-01,  4.9304e-01,  1.6843e-01,  1.3743e-01,  2.9523e-01,\n",
      "         1.1244e-01,  1.7771e-01,  3.2505e-01,  5.6106e-01,  4.6260e-01,\n",
      "         4.6137e-01,  2.4504e-01,  2.9005e-01,  9.3747e-02,  2.8382e-01,\n",
      "         7.0185e-02,  3.5034e-01,  1.8312e-01,  4.4687e-01,  3.2452e-01,\n",
      "        -7.1725e-02,  3.1605e-01,  4.7851e-01,  2.5963e-01,  3.4627e-01,\n",
      "         3.3752e-01,  4.8843e-01, -2.4215e-01, -1.7073e-01, -6.6257e-02,\n",
      "        -2.0184e-01, -9.7605e-02, -1.8414e-01,  1.6800e-01, -2.1216e-01,\n",
      "        -1.9338e-02, -1.9415e-01,  9.4590e-02, -1.2733e-01,  5.5313e-02,\n",
      "        -1.7150e-01, -2.1608e-01, -1.7905e-01,  1.7293e-01, -8.1488e-02,\n",
      "        -8.1140e-02, -1.9220e-01,  6.9524e-03,  7.0193e-03,  6.2774e-02,\n",
      "         6.2126e-02,  5.2812e-02,  6.3166e-02, -2.4380e-01, -8.3943e-02,\n",
      "         3.3080e-02,  3.2335e-02,  1.3026e-01, -1.2036e-01,  6.4072e-02,\n",
      "         5.3304e-02,  3.8441e-02,  6.4949e-02,  9.2787e-02, -8.0289e-02,\n",
      "         1.4817e-01, -6.5372e-02,  7.6862e-02, -1.0554e-01, -4.2856e-04,\n",
      "        -6.5254e-02, -7.8214e-02,  1.3738e-01,  8.6898e-02, -1.6220e-01,\n",
      "        -6.5013e-02,  7.9356e-02,  9.9142e-02, -7.8904e-02,  3.3602e-02,\n",
      "         1.1152e-01,  4.0677e-02, -3.3403e-02,  3.8794e-02, -1.2988e-01,\n",
      "        -3.6478e-02, -1.5111e-01, -5.3883e-02,  1.4723e-01,  4.2271e-02,\n",
      "         8.4983e-03,  3.3977e-01,  2.9071e-01,  4.0921e-01,  3.3640e-01,\n",
      "         1.4954e-01,  8.5871e-02,  4.1080e-01,  2.1197e-01,  4.5100e-01,\n",
      "         1.9765e-01,  5.1382e-02,  2.3585e-01,  3.9456e-01,  1.7194e-01,\n",
      "         5.4588e-01,  1.8723e-01,  4.7445e-01,  2.6845e-01,  2.9319e-01,\n",
      "         3.2327e-01,  1.9276e-01,  1.9834e-01,  1.4171e-01,  4.3690e-01,\n",
      "         4.2551e-01,  2.3325e-01,  1.1314e-01,  4.2393e-01,  8.0397e-02,\n",
      "         3.9708e-01,  2.6327e-01,  3.8917e-01], device='cuda:0')\n",
      "tensor([[-0.6500, -0.4926, -0.4778, -0.5471,  0.3315, -0.4240,  0.2048, -0.4082,\n",
      "          0.3518,  0.4034, -0.1795, -0.3345,  0.4342,  0.4827,  0.6153, -0.5856,\n",
      "          0.1920,  0.4820, -0.2643, -0.5331, -0.3048,  0.4161, -0.3360, -0.5716,\n",
      "         -0.5543,  0.1553,  0.4614,  0.4040, -0.1709,  0.3014, -0.1919, -0.5432]],\n",
      "       device='cuda:0')\n",
      "tensor([-0.0699], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(training_loss_epoches))\n",
    "plt.plot(np.linspace(1, len(training_loss_epoches), len(training_loss_epoches)).astype(int), training_loss_epoches)\n",
    "torch.save(net, 'model_task2_finalised')\n",
    "torch.save(net.state_dict(), 'model_param_task2_finalised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(19837, 600)\n",
       "  (lstm): LSTM(600, 32, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# net.load_state_dict(torch.load('model_param_task2')) #GPU\n",
    "net = torch.load('model_task2_finalised')\n",
    "# net.load_state_dict(torch.load('model_param_task2_final',map_location=torch.device('cpu'))) #for salman CPU only\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [-6.6557e-03,  7.0117e-03, -2.8231e-03,  ...,  7.2872e-03,\n",
      "          3.9233e-04, -3.9099e-03],\n",
      "        [ 2.3458e-03, -3.2887e-03, -3.0864e-03,  ...,  5.5859e-03,\n",
      "          7.9610e-03, -1.1799e-04],\n",
      "        ...,\n",
      "        [-1.5140e-03, -3.0222e-03, -5.4241e-03,  ...,  1.3369e-03,\n",
      "         -5.6326e-04,  7.2260e-03],\n",
      "        [-1.2677e-03, -3.2872e-03, -2.2281e-03,  ...,  3.7003e-03,\n",
      "         -3.3115e-03, -3.5441e-03],\n",
      "        [ 3.2113e-03,  1.9682e-05,  1.4377e-03,  ..., -2.3011e-03,\n",
      "         -3.5223e-03, -1.9228e-03]], device='cuda:0')\n",
      "tensor([[ 0.2521, -0.4896,  0.1144,  ...,  0.3995, -0.1455,  0.7618],\n",
      "        [ 0.4981, -0.2003,  0.1955,  ...,  0.1075, -0.0539,  1.2503],\n",
      "        [-0.1223, -0.1559, -0.1464,  ...,  0.2404, -0.0912,  0.3481],\n",
      "        ...,\n",
      "        [ 0.2038,  0.2649, -0.1377,  ...,  0.0828,  0.0068,  0.3901],\n",
      "        [ 0.2663, -0.0365, -0.2142,  ..., -0.2389,  0.1267,  0.4182],\n",
      "        [ 0.3970, -0.3247,  0.2147,  ...,  0.2047,  0.1630,  0.7020]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.0792,  0.3064, -0.4214,  ..., -0.0308,  0.2708,  0.1715],\n",
      "        [ 0.0159, -0.1279, -0.0209,  ...,  0.0423,  0.0921,  0.0880],\n",
      "        [ 0.0050,  0.0016, -0.2904,  ..., -0.2142, -0.1345, -0.0804],\n",
      "        ...,\n",
      "        [ 0.0398, -0.1444, -0.4302,  ..., -0.0108,  0.1379, -0.3384],\n",
      "        [ 0.0962, -0.2051, -0.0028,  ...,  0.1780,  0.0888,  0.1109],\n",
      "        [-0.0322, -0.0316, -0.3278,  ...,  0.0544, -0.0682,  0.0805]],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.2025,  0.2154, -0.0440,  0.3215, -0.0854,  0.0675,  0.1712,  0.2478,\n",
      "         0.1875,  0.0711,  0.1432, -0.1155,  0.0224, -0.0667,  0.3332,  0.2544,\n",
      "         0.0123,  0.1316,  0.1624,  0.0426,  0.0473,  0.5693,  0.0837,  0.1938,\n",
      "         0.0202,  0.3167,  0.3120,  0.0642, -0.0113,  0.4386,  0.0229,  0.3788,\n",
      "         0.2180,  0.0568,  0.0188,  0.1243,  0.0966,  0.1771,  0.1026,  0.3052,\n",
      "         0.1596, -0.0517, -0.0952,  0.0235, -0.0860, -0.0659,  0.0301, -0.0343,\n",
      "         0.1458,  0.0777, -0.0209,  0.1419,  0.2980,  0.2692,  0.0851,  0.0202,\n",
      "        -0.0468,  0.2058,  0.2423,  0.1124, -0.1765,  0.0421,  0.0786,  0.0375,\n",
      "         0.0120,  0.0640, -0.0037,  0.1099, -0.0032, -0.1281,  0.1692,  0.0750,\n",
      "         0.1329,  0.0439, -0.1195, -0.1530,  0.1080, -0.0322,  0.0758,  0.0194,\n",
      "         0.1293, -0.0069,  0.0527, -0.0612,  0.0696, -0.0941,  0.0698,  0.0327,\n",
      "        -0.0237,  0.0665,  0.0150,  0.0680,  0.0037, -0.0222, -0.0122, -0.0408,\n",
      "         0.3605,  0.2474,  0.2429,  0.1741, -0.1120,  0.0771,  0.0893,  0.1083,\n",
      "        -0.0431,  0.0252, -0.1637,  0.0373,  0.0970,  0.0100,  0.3283,  0.1560,\n",
      "        -0.0990,  0.1081,  0.0103,  0.1955, -0.0071,  0.4932, -0.0647,  0.1280,\n",
      "        -0.1093,  0.4534,  0.2806,  0.1348,  0.0402,  0.5028, -0.0754,  0.2419],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.2960, -0.0965,  0.0331,  0.2218, -0.0633, -0.0395,  0.2456,  0.0331,\n",
      "         0.0116,  0.0936, -0.0114,  0.0344,  0.1902, -0.1193,  0.3677,  0.2612,\n",
      "        -0.0090, -0.0113, -0.0772,  0.0398, -0.0141,  0.6992,  0.1562, -0.0271,\n",
      "        -0.0409,  0.5139,  0.1830,  0.0714,  0.1964,  0.4678, -0.0715,  0.1613,\n",
      "         0.1132,  0.0525,  0.1045,  0.0444, -0.1279, -0.0854, -0.0286,  0.0700,\n",
      "        -0.1243,  0.0961, -0.0203, -0.0283,  0.0743, -0.0748,  0.1988,  0.0736,\n",
      "        -0.0269,  0.1226,  0.1200,  0.2281,  0.1748,  0.0935,  0.0319,  0.0476,\n",
      "        -0.0243,  0.1280,  0.0013,  0.1645, -0.1621, -0.0282,  0.1277,  0.0628,\n",
      "        -0.0274, -0.0383, -0.0595, -0.1080, -0.0543,  0.1187, -0.1544, -0.0835,\n",
      "        -0.1245, -0.0063,  0.1284,  0.1563, -0.0991, -0.0858,  0.0026, -0.0154,\n",
      "        -0.0939, -0.0019, -0.0866,  0.1002, -0.0069,  0.0826, -0.0021, -0.0195,\n",
      "         0.0269, -0.0786, -0.0156, -0.0422, -0.0297,  0.0029,  0.0517,  0.0603,\n",
      "         0.1924,  0.0242,  0.2382,  0.2031,  0.0987,  0.1557,  0.1638,  0.1171,\n",
      "         0.1792, -0.0036,  0.1088, -0.0816, -0.0285, -0.1597,  0.1320,  0.0583,\n",
      "         0.0763, -0.0400, -0.0811,  0.2498, -0.0295,  0.7414,  0.1948, -0.0248,\n",
      "        -0.1456,  0.2534,  0.2862, -0.0468,  0.1999,  0.3499, -0.0154,  0.3025],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.3383,  0.1528, -0.2163,  ..., -0.1476,  0.1046, -0.1668],\n",
      "        [ 0.1025, -0.2864, -0.2343,  ..., -0.1404,  0.1879,  0.1522],\n",
      "        [-0.0719, -0.0858, -0.1284,  ..., -0.0595, -0.0134, -0.0920],\n",
      "        ...,\n",
      "        [-0.0198,  0.1195, -0.1663,  ..., -0.2138, -0.0538, -0.3793],\n",
      "        [-0.4519,  0.3666, -0.2861,  ..., -0.2764,  0.2634, -0.6608],\n",
      "        [ 0.1860, -0.1516,  0.0503,  ..., -0.7415,  0.0052, -0.1824]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.1069, -0.1978,  0.2115,  ..., -0.0121,  0.0323, -0.3154],\n",
      "        [-0.1100,  0.1178, -0.0510,  ..., -0.2066, -0.1217, -0.0368],\n",
      "        [-0.0346, -0.0985, -0.0834,  ...,  0.1124, -0.2067, -0.1468],\n",
      "        ...,\n",
      "        [-0.1588, -0.0934,  0.0837,  ...,  0.0358,  0.0111, -0.3555],\n",
      "        [-0.0631, -0.0342,  0.0477,  ...,  0.0826, -0.0297, -0.3973],\n",
      "        [ 0.1520, -0.1563, -0.0874,  ...,  0.0262, -0.2168, -0.1209]],\n",
      "       device='cuda:0')\n",
      "tensor([ 4.6739e-01,  1.6550e-01,  2.5403e-01,  1.2626e-01,  1.8181e-01,\n",
      "         3.8797e-01,  5.3529e-01,  3.9903e-01,  2.4217e-01,  1.6699e-01,\n",
      "         4.6136e-01,  5.6035e-01,  8.8937e-02,  4.1851e-01,  3.4731e-01,\n",
      "         5.5624e-01,  2.8566e-01,  3.3826e-01,  2.8304e-01,  3.5515e-01,\n",
      "         1.9238e-01,  2.6588e-01,  1.7602e-01,  1.1719e-01,  1.7562e-01,\n",
      "         3.2310e-01,  2.3936e-01,  1.6109e-02,  2.4766e-01,  2.6268e-02,\n",
      "         3.2180e-01,  2.7881e-01,  1.1440e-01, -3.9690e-02, -1.5495e-01,\n",
      "        -1.3710e-01,  1.3113e-01, -3.2695e-01, -2.7127e-01,  2.0026e-01,\n",
      "         3.5458e-02,  1.2875e-01, -2.0062e-01, -1.4042e-01,  5.3534e-02,\n",
      "         5.1688e-02, -9.1430e-02,  6.9726e-02, -2.5544e-01,  5.7750e-02,\n",
      "        -1.3795e-01,  1.0300e-01, -2.2810e-02,  7.5183e-02, -2.0114e-01,\n",
      "        -1.2037e-01,  7.0297e-02, -1.7781e-01, -1.5699e-01, -6.8386e-02,\n",
      "        -5.2196e-02,  2.5236e-01, -5.4875e-02, -1.0010e-01, -1.7844e-01,\n",
      "         1.0356e-01,  8.1597e-02,  1.4924e-01,  2.6045e-01,  1.5074e-01,\n",
      "         7.1523e-02, -1.0629e-01,  1.0810e-01,  9.7828e-02, -2.0310e-02,\n",
      "        -1.4358e-01, -1.4890e-01,  4.7554e-04, -1.8166e-01,  1.6538e-02,\n",
      "         4.3152e-02,  1.0661e-01,  1.0014e-01, -7.2151e-02,  1.0249e-01,\n",
      "         1.1734e-01,  1.8892e-01,  2.4129e-02, -3.0425e-02,  4.5228e-02,\n",
      "         3.0094e-02, -1.3567e-01,  6.1238e-02,  1.4347e-02, -1.0699e-01,\n",
      "         4.6046e-02,  3.3478e-01,  1.6670e-01,  3.6136e-01,  1.0331e-01,\n",
      "         2.5922e-01,  3.1046e-01,  4.8755e-01,  5.7195e-01,  3.0394e-01,\n",
      "         5.3486e-02,  3.7530e-01,  1.7377e-01,  1.7763e-01,  2.7854e-01,\n",
      "         6.0225e-01,  1.3650e-01,  4.3249e-01,  3.3071e-01,  3.1712e-01,\n",
      "         9.4965e-02,  4.9192e-01,  2.6811e-01,  1.9221e-01,  3.8395e-01,\n",
      "         2.1174e-02,  4.1833e-01,  4.0695e-01,  2.3144e-01,  2.4310e-01,\n",
      "         9.7590e-02,  3.8289e-01,  2.2504e-01], device='cuda:0')\n",
      "tensor([ 0.3118,  0.3604,  0.1479,  0.3388,  0.0356,  0.5045,  0.5574,  0.4267,\n",
      "         0.4940,  0.3498,  0.6143,  0.5462,  0.3720,  0.4001,  0.5121,  0.4699,\n",
      "         0.2380,  0.2022,  0.2044,  0.4292,  0.4012,  0.3235,  0.0956,  0.2182,\n",
      "         0.1428,  0.4323,  0.3851,  0.2464,  0.3660,  0.2884,  0.2493,  0.1492,\n",
      "        -0.1597,  0.0046, -0.0467, -0.0477,  0.1941, -0.1043, -0.0817,  0.0704,\n",
      "        -0.1588,  0.1073, -0.2614,  0.0251, -0.1683, -0.1472,  0.1003, -0.0263,\n",
      "        -0.4213,  0.1229,  0.0248, -0.0542, -0.2627, -0.1159, -0.1197, -0.2049,\n",
      "         0.1713, -0.1934, -0.2505,  0.1127, -0.2422,  0.2609,  0.1444,  0.1256,\n",
      "         0.0672, -0.1327, -0.0880, -0.0388,  0.1927, -0.0458, -0.0979, -0.0735,\n",
      "        -0.0800, -0.0067,  0.0479,  0.1784,  0.0807,  0.0104,  0.0795,  0.0149,\n",
      "         0.0415,  0.1488,  0.0784, -0.1240,  0.0899,  0.0417,  0.0692,  0.1327,\n",
      "         0.1759,  0.0440, -0.0813,  0.0850,  0.0736, -0.0828,  0.0308, -0.1167,\n",
      "         0.3156,  0.1859,  0.1339,  0.2392,  0.1780,  0.1608,  0.3739,  0.2742,\n",
      "         0.3490,  0.3584,  0.2899,  0.4785,  0.3078,  0.4921,  0.2804,  0.2430,\n",
      "         0.1901,  0.5758,  0.1742,  0.3369,  0.2299,  0.0914,  0.0786,  0.0659,\n",
      "         0.1749,  0.4463,  0.3495,  0.0274,  0.1579,  0.0314,  0.5056, -0.0201],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.5833, -0.3929, -0.4824,  0.6606, -0.1868,  0.5400, -0.6457, -0.0385,\n",
      "          0.7030,  0.4125, -0.6804, -0.4598, -0.4875,  0.6667,  0.6359, -0.6064,\n",
      "          0.5341,  0.4047, -0.6425, -0.3361, -0.5953,  0.4409, -0.3483, -0.4596,\n",
      "         -0.2764,  0.6790, -0.6316, -0.3686, -0.4821,  0.3564, -0.6346,  0.3481]],\n",
      "       device='cuda:0')\n",
      "tensor([0.0385], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for param in net.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8122\n",
      "Test Accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size, device)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    try:\n",
    "        test_output, test_h = net(inputs.to(device), batch_size)\n",
    "    except IndexError:\n",
    "        print(inputs)\n",
    "#     print(labels.dtype, test_output.dtype)\n",
    "#     print(inputs)\n",
    "    loss = criterion(test_output.detach().to(device), labels.float().to(device))\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds).to(device))\n",
    "    correct = np.squeeze(correct_tensor.cpu().detach().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-170579766b64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_y' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(test_y) - np.sum(test_y))\n",
    "print(np.sum(test_y))\n",
    "print(len(encoded_labels))\n",
    "print(np.sum(encoded_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Bengali Data on Hindi Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4665, 3) (4665,)\n"
     ]
    }
   ],
   "source": [
    "#Reading Bengali data\n",
    "bengali_data, labels = helper.get_bengali_data('data/bengali_hatespeech.csv')\n",
    "# print(bengali_data)\n",
    "print(bengali_data.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  4665\n",
      "Total words: 64027\n",
      "Unique words: 14482\n",
      "(4665, 402)\n"
     ]
    }
   ],
   "source": [
    "## Stopwords Removal of Bengali Data\n",
    "bengali_sentences = helper.apply_stopword_removal(bengali_data)\n",
    "print(\"Number of sentences: \" , len(bengali_sentences))\n",
    "\n",
    "## Building Vocabulary\n",
    "bengali_V, bengali_non_unique = helper.build_vocabulary(bengali_sentences)\n",
    "print('Total words:', len(bengali_non_unique))\n",
    "print('Unique words:', len(bengali_V))\n",
    "\n",
    "## Sentence to numeric array\n",
    "x_data_bengali, max_len_curr = helper.sentence_to_numeric_arr(bengali_sentences, bengali_V)\n",
    "\n",
    "## Apply Padding\n",
    "padded = np.array(helper.padding(x_data_bengali, max_len_curr))\n",
    "print(padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#create test data to pass to the model\n",
    "test_data = TensorDataset(torch.tensor(padded), torch.tensor(labels))\n",
    "batch_size = 64\n",
    "train_loader, valid_loader, test_loader = helper.split_data_train_valid_test(padded, labels, 64)\n",
    "net, criterion= helper.initialize_SentimentLSTM_model_bengali(len(V) + 1, 600, 32, 1, 2, device, weights1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-615d560f5db7>:23: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200 Training Loss: 0.6683\n",
      "Epoch: 2/200 Step: 100 Training Loss: 0.3855 Validation Loss: 0.3404\n",
      "Epoch: 2/200 Training Loss: 0.3418\n",
      "Epoch: 3/200 Training Loss: 0.1449\n",
      "Epoch: 4/200 Step: 200 Training Loss: 0.0378 Validation Loss: 0.0274\n",
      "Epoch: 4/200 Training Loss: 0.0714\n",
      "Epoch: 5/200 Training Loss: 0.0482\n",
      "Epoch: 6/200 Step: 300 Training Loss: 0.0376 Validation Loss: 0.0237\n",
      "Epoch: 6/200 Training Loss: 0.0393\n",
      "Epoch: 7/200 Step: 400 Training Loss: 0.0718 Validation Loss: 0.0350\n",
      "Epoch: 7/200 Training Loss: 0.0216\n",
      "Epoch: 8/200 Training Loss: 0.0178\n",
      "Epoch: 9/200 Step: 500 Training Loss: 0.0285 Validation Loss: 0.0131\n",
      "Epoch: 9/200 Training Loss: 0.0165\n",
      "Epoch: 10/200 Training Loss: 0.0117\n",
      "Epoch: 11/200 Step: 600 Training Loss: 0.0026 Validation Loss: 0.0012\n",
      "Epoch: 11/200 Training Loss: 0.0097\n",
      "Epoch: 12/200 Training Loss: 0.0104\n",
      "Epoch: 13/200 Step: 700 Training Loss: 0.0038 Validation Loss: 0.0033\n",
      "Epoch: 13/200 Training Loss: 0.0081\n",
      "Epoch: 14/200 Step: 800 Training Loss: 0.0012 Validation Loss: 0.0014\n",
      "Epoch: 14/200 Training Loss: 0.0083\n",
      "Epoch: 15/200 Training Loss: 0.0069\n",
      "Epoch: 16/200 Step: 900 Training Loss: 0.0039 Validation Loss: 0.0029\n",
      "Epoch: 16/200 Training Loss: 0.0081\n",
      "Epoch: 17/200 Training Loss: 0.0080\n",
      "Epoch: 18/200 Step: 1000 Training Loss: 0.0007 Validation Loss: 0.0003\n",
      "Epoch: 18/200 Training Loss: 0.0063\n",
      "Epoch: 19/200 Step: 1100 Training Loss: 0.0016 Validation Loss: 0.0012\n",
      "Epoch: 19/200 Training Loss: 0.0066\n",
      "Epoch: 20/200 Training Loss: 0.0070\n",
      "Epoch: 21/200 Step: 1200 Training Loss: 0.0141 Validation Loss: 0.0117\n",
      "Epoch: 21/200 Training Loss: 0.0400\n",
      "Epoch: 22/200 Training Loss: 0.0285\n",
      "Epoch: 23/200 Step: 1300 Training Loss: 0.0487 Validation Loss: 0.0013\n",
      "Epoch: 23/200 Training Loss: 0.0136\n",
      "Epoch: 24/200 Training Loss: 0.0181\n",
      "Epoch: 25/200 Step: 1400 Training Loss: 0.0083 Validation Loss: 0.0042\n",
      "Epoch: 25/200 Training Loss: 0.0091\n",
      "Epoch: 26/200 Step: 1500 Training Loss: 0.0096 Validation Loss: 0.0068\n",
      "Epoch: 26/200 Training Loss: 0.0082\n",
      "Epoch: 27/200 Training Loss: 0.0051\n",
      "Epoch: 28/200 Step: 1600 Training Loss: 0.0176 Validation Loss: 0.0168\n",
      "Epoch: 28/200 Training Loss: 0.0043\n",
      "Epoch: 29/200 Training Loss: 0.0036\n",
      "Epoch: 30/200 Step: 1700 Training Loss: 0.0008 Validation Loss: 0.0005\n",
      "Epoch: 30/200 Training Loss: 0.0046\n",
      "Epoch: 31/200 Training Loss: 0.0032\n",
      "Epoch: 32/200 Step: 1800 Training Loss: 0.0005 Validation Loss: 0.0002\n",
      "Epoch: 32/200 Training Loss: 0.0038\n",
      "Epoch: 33/200 Step: 1900 Training Loss: 0.0033 Validation Loss: 0.0021\n",
      "Epoch: 33/200 Training Loss: 0.0039\n",
      "Epoch: 34/200 Training Loss: 0.0043\n",
      "Epoch: 35/200 Step: 2000 Training Loss: 0.0004 Validation Loss: 0.0001\n",
      "Epoch: 35/200 Training Loss: 0.0035\n",
      "Epoch: 36/200 Training Loss: 0.0036\n",
      "Epoch: 37/200 Step: 2100 Training Loss: 0.0024 Validation Loss: 0.0047\n",
      "Epoch: 37/200 Training Loss: 0.0032\n",
      "Epoch: 38/200 Step: 2200 Training Loss: 0.0007 Validation Loss: 0.0002\n",
      "Epoch: 38/200 Training Loss: 0.0041\n",
      "Epoch: 39/200 Training Loss: 0.0039\n",
      "Epoch: 40/200 Step: 2300 Training Loss: 0.0269 Validation Loss: 0.0311\n",
      "Epoch: 40/200 Training Loss: 0.0027\n",
      "Epoch: 41/200 Training Loss: 0.0028\n",
      "Epoch: 42/200 Step: 2400 Training Loss: 0.0004 Validation Loss: 0.0001\n",
      "Epoch: 42/200 Training Loss: 0.0025\n",
      "Epoch: 43/200 Training Loss: 0.0024\n",
      "Epoch: 44/200 Step: 2500 Training Loss: 0.0009 Validation Loss: 0.0004\n",
      "Epoch: 44/200 Training Loss: 0.0030\n",
      "Epoch: 45/200 Step: 2600 Training Loss: 0.0003 Validation Loss: 0.0001\n",
      "Epoch: 45/200 Training Loss: 0.0031\n",
      "Epoch: 46/200 Training Loss: 0.0028\n",
      "Epoch: 47/200 Step: 2700 Training Loss: 0.0008 Validation Loss: 0.0003\n",
      "Epoch: 47/200 Training Loss: 0.0025\n",
      "Epoch: 48/200 Training Loss: 0.0024\n",
      "Epoch: 49/200 Step: 2800 Training Loss: 0.0013 Validation Loss: 0.0002\n",
      "Epoch: 49/200 Training Loss: 0.0023\n",
      "Epoch: 50/200 Step: 2900 Training Loss: 0.0007 Validation Loss: 0.0001\n",
      "Epoch: 50/200 Training Loss: 0.0022\n",
      "Epoch: 51/200 Training Loss: 0.0023\n",
      "Epoch: 52/200 Step: 3000 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 52/200 Training Loss: 0.0036\n",
      "Epoch: 53/200 Training Loss: 0.0063\n",
      "Epoch: 54/200 Step: 3100 Training Loss: 0.0162 Validation Loss: 0.0193\n",
      "Epoch: 54/200 Training Loss: 0.0060\n",
      "Epoch: 55/200 Training Loss: 0.0036\n",
      "Epoch: 56/200 Step: 3200 Training Loss: 0.0005 Validation Loss: 0.0002\n",
      "Epoch: 56/200 Training Loss: 0.0036\n",
      "Epoch: 57/200 Step: 3300 Training Loss: 0.0003 Validation Loss: 0.0001\n",
      "Epoch: 57/200 Training Loss: 0.0024\n",
      "Epoch: 58/200 Training Loss: 0.0023\n",
      "Epoch: 59/200 Step: 3400 Training Loss: 0.0002 Validation Loss: 0.0001\n",
      "Epoch: 59/200 Training Loss: 0.0019\n",
      "Epoch: 60/200 Training Loss: 0.0022\n",
      "Epoch: 61/200 Step: 3500 Training Loss: 0.0023 Validation Loss: 0.0026\n",
      "Epoch: 61/200 Training Loss: 0.0025\n",
      "Epoch: 62/200 Training Loss: 0.0023\n",
      "Epoch: 63/200 Step: 3600 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 63/200 Training Loss: 0.0021\n",
      "Epoch: 64/200 Step: 3700 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 64/200 Training Loss: 0.0020\n",
      "Epoch: 65/200 Training Loss: 0.0026\n",
      "Epoch: 66/200 Step: 3800 Training Loss: 0.0004 Validation Loss: 0.0001\n",
      "Epoch: 66/200 Training Loss: 0.0017\n",
      "Epoch: 67/200 Training Loss: 0.0018\n",
      "Epoch: 68/200 Step: 3900 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 68/200 Training Loss: 0.0025\n",
      "Epoch: 69/200 Step: 4000 Training Loss: 0.0011 Validation Loss: 0.0003\n",
      "Epoch: 69/200 Training Loss: 0.0017\n",
      "Epoch: 70/200 Training Loss: 0.0017\n",
      "Epoch: 71/200 Step: 4100 Training Loss: 0.0005 Validation Loss: 0.0000\n",
      "Epoch: 71/200 Training Loss: 0.0022\n",
      "Epoch: 72/200 Training Loss: 0.0027\n",
      "Epoch: 73/200 Step: 4200 Training Loss: 0.0001 Validation Loss: 0.0001\n",
      "Epoch: 73/200 Training Loss: 0.0022\n",
      "Epoch: 74/200 Training Loss: 0.0021\n",
      "Epoch: 75/200 Step: 4300 Training Loss: 0.0008 Validation Loss: 0.0001\n",
      "Epoch: 75/200 Training Loss: 0.0019\n",
      "Epoch: 76/200 Step: 4400 Training Loss: 0.0076 Validation Loss: 0.0109\n",
      "Epoch: 76/200 Training Loss: 0.0020\n",
      "Epoch: 77/200 Training Loss: 0.0020\n",
      "Epoch: 78/200 Step: 4500 Training Loss: 0.0004 Validation Loss: 0.0001\n",
      "Epoch: 78/200 Training Loss: 0.0020\n",
      "Epoch: 79/200 Training Loss: 0.0013\n",
      "Epoch: 80/200 Step: 4600 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 80/200 Training Loss: 0.0018\n",
      "Epoch: 81/200 Training Loss: 0.0020\n",
      "Epoch: 82/200 Step: 4700 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 82/200 Training Loss: 0.0016\n",
      "Epoch: 83/200 Step: 4800 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 83/200 Training Loss: 0.0021\n",
      "Epoch: 84/200 Training Loss: 0.0020\n",
      "Epoch: 85/200 Step: 4900 Training Loss: 0.0093 Validation Loss: 0.0117\n",
      "Epoch: 85/200 Training Loss: 0.0020\n",
      "Epoch: 86/200 Training Loss: 0.0025\n",
      "Epoch: 87/200 Step: 5000 Training Loss: 0.0117 Validation Loss: 0.0151\n",
      "Epoch: 87/200 Training Loss: 0.0014\n",
      "Epoch: 88/200 Step: 5100 Training Loss: 0.0004 Validation Loss: 0.0000\n",
      "Epoch: 88/200 Training Loss: 0.0016\n",
      "Epoch: 89/200 Training Loss: 0.0022\n",
      "Epoch: 90/200 Step: 5200 Training Loss: 0.0069 Validation Loss: 0.0085\n",
      "Epoch: 90/200 Training Loss: 0.0020\n",
      "Epoch: 91/200 Training Loss: 0.0019\n",
      "Epoch: 92/200 Step: 5300 Training Loss: 0.0076 Validation Loss: 0.0138\n",
      "Epoch: 92/200 Training Loss: 0.0021\n",
      "Epoch: 93/200 Training Loss: 0.0025\n",
      "Epoch: 94/200 Step: 5400 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 94/200 Training Loss: 0.0019\n",
      "Epoch: 95/200 Step: 5500 Training Loss: 0.0003 Validation Loss: 0.0001\n",
      "Epoch: 95/200 Training Loss: 0.0016\n",
      "Epoch: 96/200 Training Loss: 0.0015\n",
      "Epoch: 97/200 Step: 5600 Training Loss: 0.0002 Validation Loss: 0.0001\n",
      "Epoch: 97/200 Training Loss: 0.0021\n",
      "Epoch: 98/200 Training Loss: 0.0020\n",
      "Epoch: 99/200 Step: 5700 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 99/200 Training Loss: 0.0020\n",
      "Epoch: 100/200 Step: 5800 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 100/200 Training Loss: 0.0020\n",
      "Epoch: 101/200 Training Loss: 0.0017\n",
      "Epoch: 102/200 Step: 5900 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 102/200 Training Loss: 0.0015\n",
      "Epoch: 103/200 Training Loss: 0.0020\n",
      "Epoch: 104/200 Step: 6000 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 104/200 Training Loss: 0.0016\n",
      "Epoch: 105/200 Training Loss: 0.0018\n",
      "Epoch: 106/200 Step: 6100 Training Loss: 0.0004 Validation Loss: 0.0000\n",
      "Epoch: 106/200 Training Loss: 0.0014\n",
      "Epoch: 107/200 Step: 6200 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 107/200 Training Loss: 0.0017\n",
      "Epoch: 108/200 Training Loss: 0.0028\n",
      "Epoch: 109/200 Step: 6300 Training Loss: 0.0154 Validation Loss: 0.0225\n",
      "Epoch: 109/200 Training Loss: 0.0018\n",
      "Epoch: 110/200 Training Loss: 0.0016\n",
      "Epoch: 111/200 Step: 6400 Training Loss: 0.0007 Validation Loss: 0.0000\n",
      "Epoch: 111/200 Training Loss: 0.0017\n",
      "Epoch: 112/200 Training Loss: 0.0017\n",
      "Epoch: 113/200 Step: 6500 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 113/200 Training Loss: 0.0014\n",
      "Epoch: 114/200 Step: 6600 Training Loss: 0.0152 Validation Loss: 0.0114\n",
      "Epoch: 114/200 Training Loss: 0.0017\n",
      "Epoch: 115/200 Training Loss: 0.0018\n",
      "Epoch: 116/200 Step: 6700 Training Loss: 0.0014 Validation Loss: 0.0001\n",
      "Epoch: 116/200 Training Loss: 0.0020\n",
      "Epoch: 117/200 Training Loss: 0.0015\n",
      "Epoch: 118/200 Step: 6800 Training Loss: 0.0003 Validation Loss: 0.0001\n",
      "Epoch: 118/200 Training Loss: 0.0019\n",
      "Epoch: 119/200 Step: 6900 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 119/200 Training Loss: 0.0015\n",
      "Epoch: 120/200 Training Loss: 0.0020\n",
      "Epoch: 121/200 Step: 7000 Training Loss: 0.0001 Validation Loss: 0.0001\n",
      "Epoch: 121/200 Training Loss: 0.0017\n",
      "Epoch: 122/200 Training Loss: 0.0019\n",
      "Epoch: 123/200 Step: 7100 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 123/200 Training Loss: 0.0022\n",
      "Epoch: 124/200 Training Loss: 0.0019\n",
      "Epoch: 125/200 Step: 7200 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 125/200 Training Loss: 0.0016\n",
      "Epoch: 126/200 Step: 7300 Training Loss: 0.0002 Validation Loss: 0.0002\n",
      "Epoch: 126/200 Training Loss: 0.0017\n",
      "Epoch: 127/200 Training Loss: 0.0015\n",
      "Epoch: 128/200 Step: 7400 Training Loss: 0.0060 Validation Loss: 0.0043\n",
      "Epoch: 128/200 Training Loss: 0.0022\n",
      "Epoch: 129/200 Training Loss: 0.0017\n",
      "Epoch: 130/200 Step: 7500 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 130/200 Training Loss: 0.0017\n",
      "Epoch: 131/200 Training Loss: 0.0016\n",
      "Epoch: 132/200 Step: 7600 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 132/200 Training Loss: 0.0016\n",
      "Epoch: 133/200 Step: 7700 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 133/200 Training Loss: 0.0015\n",
      "Epoch: 134/200 Training Loss: 0.0014\n",
      "Epoch: 135/200 Step: 7800 Training Loss: 0.0004 Validation Loss: 0.0000\n",
      "Epoch: 135/200 Training Loss: 0.0018\n",
      "Epoch: 136/200 Training Loss: 0.0017\n",
      "Epoch: 137/200 Step: 7900 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 137/200 Training Loss: 0.0016\n",
      "Epoch: 138/200 Step: 8000 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 138/200 Training Loss: 0.0013\n",
      "Epoch: 139/200 Training Loss: 0.0016\n",
      "Epoch: 140/200 Step: 8100 Training Loss: 0.0002 Validation Loss: 0.0001\n",
      "Epoch: 140/200 Training Loss: 0.0015\n",
      "Epoch: 141/200 Training Loss: 0.0020\n",
      "Epoch: 142/200 Step: 8200 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 142/200 Training Loss: 0.0015\n",
      "Epoch: 143/200 Training Loss: 0.0018\n",
      "Epoch: 144/200 Step: 8300 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 144/200 Training Loss: 0.0019\n",
      "Epoch: 145/200 Step: 8400 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 145/200 Training Loss: 0.0016\n",
      "Epoch: 146/200 Training Loss: 0.0012\n",
      "Epoch: 147/200 Step: 8500 Training Loss: 0.0002 Validation Loss: 0.0001\n",
      "Epoch: 147/200 Training Loss: 0.0014\n",
      "Epoch: 148/200 Training Loss: 0.0019\n",
      "Epoch: 149/200 Step: 8600 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 149/200 Training Loss: 0.0014\n",
      "Epoch: 150/200 Step: 8700 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 150/200 Training Loss: 0.0019\n",
      "Epoch: 151/200 Training Loss: 0.0014\n",
      "Epoch: 152/200 Step: 8800 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 152/200 Training Loss: 0.0015\n",
      "Epoch: 153/200 Training Loss: 0.0015\n",
      "Epoch: 154/200 Step: 8900 Training Loss: 0.0069 Validation Loss: 0.0045\n",
      "Epoch: 154/200 Training Loss: 0.0012\n",
      "Epoch: 155/200 Training Loss: 0.0012\n",
      "Epoch: 156/200 Step: 9000 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 156/200 Training Loss: 0.0010\n",
      "Epoch: 157/200 Step: 9100 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 157/200 Training Loss: 0.0021\n",
      "Epoch: 158/200 Training Loss: 0.0015\n",
      "Epoch: 159/200 Step: 9200 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 159/200 Training Loss: 0.0014\n",
      "Epoch: 160/200 Training Loss: 0.0013\n",
      "Epoch: 161/200 Step: 9300 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 161/200 Training Loss: 0.0016\n",
      "Epoch: 162/200 Training Loss: 0.0014\n",
      "Epoch: 163/200 Step: 9400 Training Loss: 0.0004 Validation Loss: 0.0000\n",
      "Epoch: 163/200 Training Loss: 0.0025\n",
      "Epoch: 164/200 Step: 9500 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 164/200 Training Loss: 0.0012\n",
      "Epoch: 165/200 Training Loss: 0.0017\n",
      "Epoch: 166/200 Step: 9600 Training Loss: 0.0211 Validation Loss: 0.0140\n",
      "Epoch: 166/200 Training Loss: 0.0015\n",
      "Epoch: 167/200 Training Loss: 0.0016\n",
      "Epoch: 168/200 Step: 9700 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 168/200 Training Loss: 0.0017\n",
      "Epoch: 169/200 Step: 9800 Training Loss: 0.0002 Validation Loss: 0.0000\n",
      "Epoch: 169/200 Training Loss: 0.0017\n",
      "Epoch: 170/200 Training Loss: 0.0015\n",
      "Epoch: 171/200 Step: 9900 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 171/200 Training Loss: 0.0014\n",
      "Epoch: 172/200 Training Loss: 0.0014\n",
      "Epoch: 173/200 Step: 10000 Training Loss: 0.0005 Validation Loss: 0.0000\n",
      "Epoch: 173/200 Training Loss: 0.0017\n",
      "Epoch: 174/200 Training Loss: 0.0016\n",
      "Epoch: 175/200 Step: 10100 Training Loss: 0.0166 Validation Loss: 0.0118\n",
      "Epoch: 175/200 Training Loss: 0.0020\n",
      "Epoch: 176/200 Step: 10200 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 176/200 Training Loss: 0.0018\n",
      "Epoch: 177/200 Training Loss: 0.0015\n",
      "Epoch: 178/200 Step: 10300 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 178/200 Training Loss: 0.0014\n",
      "Epoch: 179/200 Training Loss: 0.0016\n",
      "Epoch: 180/200 Step: 10400 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 180/200 Training Loss: 0.0016\n",
      "Epoch: 181/200 Training Loss: 0.0016\n",
      "Epoch: 182/200 Step: 10500 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 182/200 Training Loss: 0.0016\n",
      "Epoch: 183/200 Step: 10600 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 183/200 Training Loss: 0.0015\n",
      "Epoch: 184/200 Training Loss: 0.0016\n",
      "Epoch: 185/200 Step: 10700 Training Loss: 0.0004 Validation Loss: 0.0000\n",
      "Epoch: 185/200 Training Loss: 0.0017\n",
      "Epoch: 186/200 Training Loss: 0.0018\n",
      "Epoch: 187/200 Step: 10800 Training Loss: 0.0004 Validation Loss: 0.0002\n",
      "Epoch: 187/200 Training Loss: 0.0014\n",
      "Epoch: 188/200 Step: 10900 Training Loss: 0.0196 Validation Loss: 0.0115\n",
      "Epoch: 188/200 Training Loss: 0.0014\n",
      "Epoch: 189/200 Training Loss: 0.0016\n",
      "Epoch: 190/200 Step: 11000 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 190/200 Training Loss: 0.0015\n",
      "Epoch: 191/200 Training Loss: 0.0014\n",
      "Epoch: 192/200 Step: 11100 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 192/200 Training Loss: 0.0017\n",
      "Epoch: 193/200 Training Loss: 0.0014\n",
      "Epoch: 194/200 Step: 11200 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 194/200 Training Loss: 0.0015\n",
      "Epoch: 195/200 Step: 11300 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 195/200 Training Loss: 0.0017\n",
      "Epoch: 196/200 Training Loss: 0.0019\n",
      "Epoch: 197/200 Step: 11400 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 197/200 Training Loss: 0.0014\n",
      "Epoch: 198/200 Training Loss: 0.0015\n",
      "Epoch: 199/200 Step: 11500 Training Loss: 0.0001 Validation Loss: 0.0000\n",
      "Epoch: 199/200 Training Loss: 0.0013\n",
      "Epoch: 200/200 Step: 11600 Training Loss: 0.0000 Validation Loss: 0.0000\n",
      "Epoch: 200/200 Training Loss: 0.0017\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001, amsgrad=True)\n",
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 200  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "\n",
    "training_loss_epoches = []\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size, device)\n",
    "    training_loss = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         print(i, inputs.shape, labels.shape)\n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs, batch_size)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        training_loss.append(loss.item())\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            v_h = net.init_hidden(batch_size, device)\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "                v_output, v_h = net(v_inputs, batch_size)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()\n",
    "    training_loss_epoches.append(np.mean(training_loss))\n",
    "    print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "          \"Training Loss: {:.4f}\".format(np.mean(training_loss)))\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd6215fdd90>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# net, criterion= helper.initialize_SentimentLSTM_model(len(V) + 1, 600, 32, 1, 2, device, weights1)\n",
    "plt.plot(np.linspace(1, len(training_loss_epoches), len(training_loss_epoches)).astype(int), training_loss_epoches)\n",
    "\n",
    "# net.load_state_dict(torch.load('model_param_task2_test')) #GPU\n",
    "\n",
    "# net.load_state_dict(torch.load('model_param_task2_final',map_location=torch.device('cpu'))) #for salman CPU only\n",
    "# net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.0306\n",
      "Test Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size, device)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    try:\n",
    "        test_output, test_h = net(inputs.to(device), batch_size)\n",
    "    except IndexError:\n",
    "        print(inputs)\n",
    "#     print(labels.dtype, test_output.dtype)\n",
    "#     print(inputs)\n",
    "    loss = criterion(test_output.detach().to(device), labels.float().to(device))\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds).to(device))\n",
    "    correct = np.squeeze(correct_tensor.cpu().detach().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training classifier on Bengali embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(\n",
      "  (fc1): Linear(in_features=14482, out_features=600, bias=True)\n",
      "  (fc2): Linear(in_features=600, out_features=14482, bias=True)\n",
      ")\n",
      "torch.Size([14483, 600]) torch.Size([600, 14482])\n"
     ]
    }
   ],
   "source": [
    "#Load Word2Vec embeddings module\n",
    "weights1, weights2 = helper.load_word2vec_embeddings('model_param_finalised_bengali', device, len(bengali_V), 600)\n",
    "print(weights1.shape, weights2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4665, 402) torch.Size([64])\n",
      "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#split data into train, valid & test set\n",
    "# labels = labels.cpu().detach().numpy()\n",
    "print(padded.shape, labels.shape)\n",
    "print(type(padded), type(labels))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(14483, 600)\n",
      "  (lstm): LSTM(600, 32, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net, criterion = helper.initialize_SentimentLSTM_model(len(bengali_V) + 1, 600, 32, 1, 2, device, weights1)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-4a2a9920d6d9>:28: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Training Loss: 0.6926\n",
      "Epoch: 2/100 Step: 100 Training Loss: 0.6556 Validation Loss: 0.6580\n",
      "Epoch: 2/100 Training Loss: 0.6688\n",
      "Epoch: 3/100 Training Loss: 0.5723\n",
      "Epoch: 4/100 Step: 200 Training Loss: 0.4564 Validation Loss: 0.4169\n",
      "Epoch: 4/100 Training Loss: 0.4927\n",
      "Epoch: 5/100 Training Loss: 0.4381\n",
      "Epoch: 6/100 Step: 300 Training Loss: 0.5123 Validation Loss: 0.4533\n",
      "Epoch: 6/100 Training Loss: 0.3970\n",
      "Epoch: 7/100 Step: 400 Training Loss: 0.4728 Validation Loss: 0.4199\n",
      "Epoch: 7/100 Training Loss: 0.3559\n",
      "Epoch: 8/100 Training Loss: 0.3093\n",
      "Epoch: 9/100 Step: 500 Training Loss: 0.4244 Validation Loss: 0.3558\n",
      "Epoch: 9/100 Training Loss: 0.2688\n",
      "Epoch: 10/100 Training Loss: 0.2294\n",
      "Epoch: 11/100 Step: 600 Training Loss: 0.2364 Validation Loss: 0.1606\n",
      "Epoch: 11/100 Training Loss: 0.2078\n",
      "Epoch: 12/100 Training Loss: 0.1795\n",
      "Epoch: 13/100 Step: 700 Training Loss: 0.1443 Validation Loss: 0.1387\n",
      "Epoch: 13/100 Training Loss: 0.1614\n",
      "Epoch: 14/100 Step: 800 Training Loss: 0.0869 Validation Loss: 0.0598\n",
      "Epoch: 14/100 Training Loss: 0.1453\n",
      "Epoch: 15/100 Training Loss: 0.1394\n",
      "Epoch: 16/100 Step: 900 Training Loss: 0.0546 Validation Loss: 0.0361\n",
      "Epoch: 16/100 Training Loss: 0.1175\n",
      "Epoch: 17/100 Training Loss: 0.1034\n",
      "Epoch: 18/100 Step: 1000 Training Loss: 0.1100 Validation Loss: 0.0916\n",
      "Epoch: 18/100 Training Loss: 0.0976\n",
      "Epoch: 19/100 Step: 1100 Training Loss: 0.1170 Validation Loss: 0.0486\n",
      "Epoch: 19/100 Training Loss: 0.0964\n",
      "Epoch: 20/100 Training Loss: 0.0778\n",
      "Epoch: 21/100 Step: 1200 Training Loss: 0.0338 Validation Loss: 0.0180\n",
      "Epoch: 21/100 Training Loss: 0.0713\n",
      "Epoch: 22/100 Training Loss: 0.0695\n",
      "Epoch: 23/100 Step: 1300 Training Loss: 0.0633 Validation Loss: 0.0262\n",
      "Epoch: 23/100 Training Loss: 0.0611\n",
      "Epoch: 24/100 Training Loss: 0.0823\n",
      "Epoch: 25/100 Step: 1400 Training Loss: 0.0644 Validation Loss: 0.0277\n",
      "Epoch: 25/100 Training Loss: 0.0752\n",
      "Epoch: 26/100 Step: 1500 Training Loss: 0.1085 Validation Loss: 0.1179\n",
      "Epoch: 26/100 Training Loss: 0.0743\n",
      "Epoch: 27/100 Training Loss: 0.0700\n",
      "Epoch: 28/100 Step: 1600 Training Loss: 0.1386 Validation Loss: 0.0991\n",
      "Epoch: 28/100 Training Loss: 0.0508\n",
      "Epoch: 29/100 Training Loss: 0.0519\n",
      "Epoch: 30/100 Step: 1700 Training Loss: 0.0581 Validation Loss: 0.0438\n",
      "Epoch: 30/100 Training Loss: 0.0471\n",
      "Epoch: 31/100 Training Loss: 0.0409\n",
      "Epoch: 32/100 Step: 1800 Training Loss: 0.0316 Validation Loss: 0.0219\n",
      "Epoch: 32/100 Training Loss: 0.0447\n",
      "Epoch: 33/100 Step: 1900 Training Loss: 0.0560 Validation Loss: 0.0052\n",
      "Epoch: 33/100 Training Loss: 0.0420\n",
      "Epoch: 34/100 Training Loss: 0.0397\n",
      "Epoch: 35/100 Step: 2000 Training Loss: 0.0103 Validation Loss: 0.0144\n",
      "Epoch: 35/100 Training Loss: 0.0392\n",
      "Epoch: 36/100 Training Loss: 0.0353\n",
      "Epoch: 37/100 Step: 2100 Training Loss: 0.1493 Validation Loss: 0.0869\n",
      "Epoch: 37/100 Training Loss: 0.0389\n",
      "Epoch: 38/100 Step: 2200 Training Loss: 0.1228 Validation Loss: 0.1147\n",
      "Epoch: 38/100 Training Loss: 0.0364\n",
      "Epoch: 39/100 Training Loss: 0.0362\n",
      "Epoch: 40/100 Step: 2300 Training Loss: 0.0078 Validation Loss: 0.0094\n",
      "Epoch: 40/100 Training Loss: 0.0439\n",
      "Epoch: 41/100 Training Loss: 0.0373\n",
      "Epoch: 42/100 Step: 2400 Training Loss: 0.0220 Validation Loss: 0.0050\n",
      "Epoch: 42/100 Training Loss: 0.0316\n",
      "Epoch: 43/100 Training Loss: 0.0396\n",
      "Epoch: 44/100 Step: 2500 Training Loss: 0.0356 Validation Loss: 0.0146\n",
      "Epoch: 44/100 Training Loss: 0.0358\n",
      "Epoch: 45/100 Step: 2600 Training Loss: 0.0410 Validation Loss: 0.0081\n",
      "Epoch: 45/100 Training Loss: 0.0356\n",
      "Epoch: 46/100 Training Loss: 0.0522\n",
      "Epoch: 47/100 Step: 2700 Training Loss: 0.0167 Validation Loss: 0.0132\n",
      "Epoch: 47/100 Training Loss: 0.0457\n",
      "Epoch: 48/100 Training Loss: 0.0255\n",
      "Epoch: 49/100 Step: 2800 Training Loss: 0.0063 Validation Loss: 0.0015\n",
      "Epoch: 49/100 Training Loss: 0.0321\n",
      "Epoch: 50/100 Step: 2900 Training Loss: 0.0026 Validation Loss: 0.0012\n",
      "Epoch: 50/100 Training Loss: 0.0270\n",
      "Epoch: 51/100 Training Loss: 0.0227\n",
      "Epoch: 52/100 Step: 3000 Training Loss: 0.0059 Validation Loss: 0.0020\n",
      "Epoch: 52/100 Training Loss: 0.0224\n",
      "Epoch: 53/100 Training Loss: 0.0194\n",
      "Epoch: 54/100 Step: 3100 Training Loss: 0.0064 Validation Loss: 0.0013\n",
      "Epoch: 54/100 Training Loss: 0.0158\n",
      "Epoch: 55/100 Training Loss: 0.0190\n",
      "Epoch: 56/100 Step: 3200 Training Loss: 0.0385 Validation Loss: 0.0024\n",
      "Epoch: 56/100 Training Loss: 0.0204\n",
      "Epoch: 57/100 Step: 3300 Training Loss: 0.0146 Validation Loss: 0.0077\n",
      "Epoch: 57/100 Training Loss: 0.0156\n",
      "Epoch: 58/100 Training Loss: 0.0174\n",
      "Epoch: 59/100 Step: 3400 Training Loss: 0.0184 Validation Loss: 0.0061\n",
      "Epoch: 59/100 Training Loss: 0.0158\n",
      "Epoch: 60/100 Training Loss: 0.0149\n",
      "Epoch: 61/100 Step: 3500 Training Loss: 0.0080 Validation Loss: 0.0006\n",
      "Epoch: 61/100 Training Loss: 0.0178\n",
      "Epoch: 62/100 Training Loss: 0.0199\n",
      "Epoch: 63/100 Step: 3600 Training Loss: 0.0120 Validation Loss: 0.0006\n",
      "Epoch: 63/100 Training Loss: 0.0140\n",
      "Epoch: 64/100 Step: 3700 Training Loss: 0.0242 Validation Loss: 0.0285\n",
      "Epoch: 64/100 Training Loss: 0.0134\n",
      "Epoch: 65/100 Training Loss: 0.0127\n",
      "Epoch: 66/100 Step: 3800 Training Loss: 0.0113 Validation Loss: 0.0064\n",
      "Epoch: 66/100 Training Loss: 0.0105\n",
      "Epoch: 67/100 Training Loss: 0.0125\n",
      "Epoch: 68/100 Step: 3900 Training Loss: 0.0030 Validation Loss: 0.0005\n",
      "Epoch: 68/100 Training Loss: 0.0131\n",
      "Epoch: 69/100 Step: 4000 Training Loss: 0.0056 Validation Loss: 0.0006\n",
      "Epoch: 69/100 Training Loss: 0.0107\n",
      "Epoch: 70/100 Training Loss: 0.0091\n",
      "Epoch: 71/100 Step: 4100 Training Loss: 0.0118 Validation Loss: 0.0020\n",
      "Epoch: 71/100 Training Loss: 0.0109\n",
      "Epoch: 72/100 Training Loss: 0.0122\n",
      "Epoch: 73/100 Step: 4200 Training Loss: 0.0027 Validation Loss: 0.0003\n",
      "Epoch: 73/100 Training Loss: 0.0146\n",
      "Epoch: 74/100 Training Loss: 0.0142\n",
      "Epoch: 75/100 Step: 4300 Training Loss: 0.0237 Validation Loss: 0.0039\n",
      "Epoch: 75/100 Training Loss: 0.0131\n",
      "Epoch: 76/100 Step: 4400 Training Loss: 0.1348 Validation Loss: 0.1186\n",
      "Epoch: 76/100 Training Loss: 0.0104\n",
      "Epoch: 77/100 Training Loss: 0.0109\n",
      "Epoch: 78/100 Step: 4500 Training Loss: 0.0045 Validation Loss: 0.0004\n",
      "Epoch: 78/100 Training Loss: 0.0133\n",
      "Epoch: 79/100 Training Loss: 0.0115\n",
      "Epoch: 80/100 Step: 4600 Training Loss: 0.0063 Validation Loss: 0.0007\n",
      "Epoch: 80/100 Training Loss: 0.0116\n",
      "Epoch: 81/100 Training Loss: 0.0096\n",
      "Epoch: 82/100 Step: 4700 Training Loss: 0.0071 Validation Loss: 0.0022\n",
      "Epoch: 82/100 Training Loss: 0.0072\n",
      "Epoch: 83/100 Step: 4800 Training Loss: 0.0029 Validation Loss: 0.0008\n",
      "Epoch: 83/100 Training Loss: 0.0065\n",
      "Epoch: 84/100 Training Loss: 0.0076\n",
      "Epoch: 85/100 Step: 4900 Training Loss: 0.0180 Validation Loss: 0.0024\n",
      "Epoch: 85/100 Training Loss: 0.0097\n",
      "Epoch: 86/100 Training Loss: 0.0121\n",
      "Epoch: 87/100 Step: 5000 Training Loss: 0.0064 Validation Loss: 0.0014\n",
      "Epoch: 87/100 Training Loss: 0.0132\n",
      "Epoch: 88/100 Step: 5100 Training Loss: 0.0015 Validation Loss: 0.0007\n",
      "Epoch: 88/100 Training Loss: 0.0124\n",
      "Epoch: 89/100 Training Loss: 0.0238\n",
      "Epoch: 90/100 Step: 5200 Training Loss: 0.0293 Validation Loss: 0.0069\n",
      "Epoch: 90/100 Training Loss: 0.0121\n",
      "Epoch: 91/100 Training Loss: 0.0193\n",
      "Epoch: 92/100 Step: 5300 Training Loss: 0.2089 Validation Loss: 0.1751\n",
      "Epoch: 92/100 Training Loss: 0.0257\n",
      "Epoch: 93/100 Training Loss: 0.0169\n",
      "Epoch: 94/100 Step: 5400 Training Loss: 0.0006 Validation Loss: 0.0002\n",
      "Epoch: 94/100 Training Loss: 0.0221\n",
      "Epoch: 95/100 Step: 5500 Training Loss: 0.0016 Validation Loss: 0.0003\n",
      "Epoch: 95/100 Training Loss: 0.0197\n",
      "Epoch: 96/100 Training Loss: 0.0113\n",
      "Epoch: 97/100 Step: 5600 Training Loss: 0.0024 Validation Loss: 0.0011\n",
      "Epoch: 97/100 Training Loss: 0.0124\n",
      "Epoch: 98/100 Training Loss: 0.0152\n",
      "Epoch: 99/100 Step: 5700 Training Loss: 0.0050 Validation Loss: 0.0003\n",
      "Epoch: 99/100 Training Loss: 0.0143\n",
      "Epoch: 100/100 Step: 5800 Training Loss: 0.0313 Validation Loss: 0.0003\n",
      "Epoch: 100/100 Training Loss: 0.0114\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001, amsgrad=True)\n",
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 100  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda:0\"\n",
    "# else:\n",
    "#     device = \"cpu\"\n",
    "# print(device)\n",
    "\n",
    "training_loss_epoches = []\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size, device)\n",
    "    training_loss = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs, batch_size)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        training_loss.append(loss.item())\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            v_h = net.init_hidden(batch_size, device)\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "                v_output, v_h = net(v_inputs, batch_size)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()\n",
    "    training_loss_epoches.append(np.mean(training_loss))\n",
    "    print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "          \"Training Loss: {:.4f}\".format(np.mean(training_loss)))\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(training_loss_epoches))\n",
    "plt.plot(np.linspace(1, len(training_loss_epoches), len(training_loss_epoches)).astype(int), training_loss_epoches)\n",
    "torch.save(net, 'model_bengali_task2')\n",
    "torch.save(net.state_dict(), 'model_param_bengali_task2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.6829\n",
      "Test Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "# import os \n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size, device)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    try:\n",
    "        test_output, test_h = net(inputs.to(device), batch_size)\n",
    "    except IndexError:\n",
    "        print(inputs)\n",
    "#     print(labels.dtype, test_output.dtype)\n",
    "#     print(inputs)\n",
    "    loss = criterion(test_output.detach().to(device), labels.float().to(device))\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    \n",
    "    \n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds).to(device))\n",
    "    correct = np.squeeze(correct_tensor.cpu().detach().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
