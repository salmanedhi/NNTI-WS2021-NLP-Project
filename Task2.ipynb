{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch as torch\n",
    "from torch import nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math as mt\n",
    "import time\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, features, embedding_size):\n",
    "        super().__init__()\n",
    "        initrange = 0.5 / embedding_size\n",
    "        self.fc1 = nn.Linear(features, embedding_size)\n",
    "        self.fc2 = nn.Linear(embedding_size, features)\n",
    "\n",
    "\n",
    "    def forward(self, one_hot):\n",
    "        x = self.fc1(one_hot.float())\n",
    "        x = self.fc2(x)\n",
    "        log_softmax = torch.nn.functional.log_softmax(x, dim=1)\n",
    "        return log_softmax\n",
    "#     pass\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cuda:0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "url = 'https://raw.githubusercontent.com/salmanedhi/NNTI-WS2021-NLP-Project/main/data/hindi_hatespeech.tsv'\n",
    "\n",
    "data = pd.read_csv(url, sep='\\t')\n",
    "    \n",
    "#data = pd.read_csv('data/hindi_hatespeech.tsv', sep='\\t')\n",
    "data_development = shuffle(data)\n",
    "labels = data_development['task_2']\n",
    "# data_development = data\n",
    "type(data_development['task_1'])\n",
    "\n",
    "print(\"Done\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "hindi_stopword_file = open('data/stopwords-hi.txt', encoding=\"utf8\")\n",
    "\n",
    "sw_list = ['#', '?', '!', ';', ',', ':', \"\\'\", '-', '=', '(', ')', '[', ']' , '{', '}', '\"', '*', '@', '  ', '\\\\', '/', '..', '...', '....', '%'\n",
    "          ,'0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '\\t']\n",
    "sw_list_string = ''\n",
    "for i in sw_list:\n",
    "    sw_list_string+=i\n",
    "hindi_stopwords = []\n",
    "for x in hindi_stopword_file:\n",
    "    hindi_stopwords.append(x.rstrip())\n",
    "\n",
    "hindi_stopwords.extend(sw_list)\n",
    "sentences = []\n",
    "for text in data_development['text']:\n",
    "    text_array = text.split(' ')\n",
    "    new_array = []\n",
    "    for j in text_array:\n",
    "        if '@' not in j and len(j) < 20:\n",
    "            for char in sw_list:\n",
    "                j = j.replace(char, '')\n",
    "            new_array.append(j.lower())\n",
    "    sentences.append(' '.join(new_array))\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 141550\n",
      "Unique words: 19836\n"
     ]
    }
   ],
   "source": [
    "#TODO: implement!\n",
    "temp_unique = [] # For unique words\n",
    "temp_nounique = []\n",
    "for j in sentences:\n",
    "    temp2 = j.split(' ')\n",
    "    for k in temp2:\n",
    "        if k not in temp_unique:\n",
    "            temp_unique.append(k)\n",
    "        temp_nounique.append(k)\n",
    "V = temp_unique\n",
    "non_unique = temp_nounique\n",
    "print('Total words:', len(non_unique))\n",
    "print('Unique words:', len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(\n",
      "  (fc1): Linear(in_features=19836, out_features=600, bias=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19836, bias=True)\n",
      ")\n",
      "torch.Size([19837, 600]) torch.Size([600, 19836])\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(19836, 600)\n",
    "model.load_state_dict(torch.load('model_param_finalised'))\n",
    "print(model.eval())\n",
    "weights1 = torch.transpose(model.fc1.weight, 0, 1)\n",
    "weights2 = torch.transpose(model.fc2.weight, 0, 1)\n",
    "temp_row = np.zeros(600).reshape(1,600)\n",
    "weights1_np = weights1.cpu().detach().numpy()\n",
    "weights1_np = np.concatenate((temp_row, weights1_np), axis=0)\n",
    "weights1 = torch.Tensor(weights1_np)\n",
    "print(weights1.shape, weights2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([1, 2, 3, 4, 5, 6, 3, 7, 8, 3, 9, 10, 11, 12, 13, 14, 15, 16, 17, 12, 18, 19, 19, 19, 20, 12, 21, 22, 19, 23, 24, 25, 26, 19, 19, 19, 19, 19, 19, 19], 40)\n",
      "4153    PRFN\n",
      "51      PRFN\n",
      "1061    PRFN\n",
      "3913    NONE\n",
      "638     PRFN\n",
      "        ... \n",
      "2918    PRFN\n",
      "4213    OFFN\n",
      "3306    NONE\n",
      "3587    NONE\n",
      "3003    HATE\n",
      "Name: task_2, Length: 4665, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#TODO: implement!\n",
    "def word_to_one_hot(word, features):\n",
    "    if word in V:\n",
    "        index = V.index(word)\n",
    "        encoding = np.zeros(features)\n",
    "        encoding[index]= 1\n",
    "        return encoding.astype(np.uint8)\n",
    "    return False\n",
    "\n",
    "def not_word_to_one_hot(word, features):\n",
    "    if word in V:\n",
    "        index = V.index(word)\n",
    "        return index\n",
    "    return False\n",
    "\n",
    "def index_to_onehot(X_batch):\n",
    "    X_batch_new = []\n",
    "    y_batch_new = []\n",
    "    encodingX = np.eye(4)[X_batch]\n",
    "    return type(encodingX)\n",
    "\n",
    "def sentence_to_index(sentence):\n",
    "    words = sentence.split(' ')\n",
    "    l = []\n",
    "    for i in words:\n",
    "        l.append(V.index(i) + 1)\n",
    "    return l, len(l)\n",
    "#     return l\n",
    "\n",
    "def padding(array, seq_len):\n",
    "    padded_array = []\n",
    "    for item in array:\n",
    "        if len(item) < seq_len:\n",
    "            padded_array.append([0]*(seq_len-len(item)) + item)\n",
    "        else:\n",
    "            padded_array.append(item)\n",
    "    return np.array(padded_array)\n",
    "            \n",
    "#     for review in encoded_reviews:\n",
    "#         if len(review) >= seq_length:\n",
    "#             reviews.append(review[:seq_length])\n",
    "#         else:\n",
    "#             reviews.append([0]*(seq_length-len(review)) + review)\n",
    "        \n",
    "#     return np.array(reviews)\n",
    "\n",
    "print(sentence_to_index(sentences[0]))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4665, 132)\n",
      "[1 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "x_data = []\n",
    "max_len_curr = -1\n",
    "for sentence in sentences:\n",
    "    temp, max_len = sentence_to_index(sentence)\n",
    "    if max_len > max_len_curr:\n",
    "        max_len_curr = max_len\n",
    "    x_data.append(temp)\n",
    "\n",
    "# print(x_data)\n",
    "padded = np.array(padding(x_data, max_len_curr))\n",
    "\n",
    "# x_data = x_data.astype('float32')\n",
    "# print(x_data.shape)\n",
    "print(padded.shape)\n",
    "encoded_labels = [0 if label == \"NONE\" else 1 for label in labels]\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "print(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (4665, 132) (466, 132) (3732, 132)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "##############  7. SPLIT DATA & GET (REVIEW, LABEL) DATALOADER  ###############\n",
    "###############################################################################\n",
    "train_ratio = 0.8\n",
    "valid_ratio = (1 - train_ratio)/2\n",
    "total = padded.shape[0]\n",
    "train_cutoff = int(total * train_ratio)\n",
    "valid_cutoff = int(total * (1 - valid_ratio))\n",
    "\n",
    "train_x, train_y = padded[:train_cutoff], encoded_labels[:train_cutoff]\n",
    "valid_x, valid_y = padded[train_cutoff : valid_cutoff], encoded_labels[train_cutoff : valid_cutoff]\n",
    "test_x, test_y = padded[valid_cutoff:], encoded_labels[valid_cutoff:]\n",
    "# print(train_x.shape, train_y.shape)\n",
    "# print(test_x.shape, test_y.shape)\n",
    "# print(valid_x.shape, valid_y.shape)\n",
    "# train_x = torch.from_numpy(train_x)\n",
    "train_data = TensorDataset(torch.tensor(train_x), torch.tensor(train_y))\n",
    "valid_data = TensorDataset(torch.tensor(valid_x), torch.tensor(valid_y))\n",
    "test_data = TensorDataset(torch.tensor(test_x), torch.tensor(test_y))\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True, drop_last=True)\n",
    "print(\"Done\", padded.shape, valid_x.shape, train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n",
    "    \n",
    "#     def __init__(self, cool):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        #embeddings is a torch tensor.\n",
    "        self.embedding.weight = nn.Parameter(weights1, requires_grad = False)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "#         print('HEre',input_words.shape)\n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19837 600 32 1 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(19837, 600)\n",
       "  (lstm): LSTM(600, 32, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = len(V) + 1\n",
    "n_embed = 600\n",
    "n_hidden = 32\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "n_layers = 2\n",
    "print(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net.to(device)\n",
    "# net = SentimentLSTM(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-60-2801870d6992>:29: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 Training Loss: 0.6923\n",
      "Epoch: 2/100 Step: 100 Training Loss: 0.6937 Validation Loss: 0.7014\n",
      "Epoch: 2/100 Training Loss: 0.6830\n",
      "Epoch: 3/100 Training Loss: 0.6730\n",
      "Epoch: 4/100 Step: 200 Training Loss: 0.6545 Validation Loss: 0.6570\n",
      "Epoch: 4/100 Training Loss: 0.6666\n",
      "Epoch: 5/100 Training Loss: 0.6589\n",
      "Epoch: 6/100 Step: 300 Training Loss: 0.6849 Validation Loss: 0.6890\n",
      "Epoch: 6/100 Training Loss: 0.6467\n",
      "Epoch: 7/100 Step: 400 Training Loss: 0.6588 Validation Loss: 0.6478\n",
      "Epoch: 7/100 Training Loss: 0.6353\n",
      "Epoch: 8/100 Training Loss: 0.6286\n",
      "Epoch: 9/100 Step: 500 Training Loss: 0.6249 Validation Loss: 0.6231\n",
      "Epoch: 9/100 Training Loss: 0.6040\n",
      "Epoch: 10/100 Training Loss: 0.5897\n",
      "Epoch: 11/100 Step: 600 Training Loss: 0.5132 Validation Loss: 0.5042\n",
      "Epoch: 11/100 Training Loss: 0.5760\n",
      "Epoch: 12/100 Training Loss: 0.5549\n",
      "Epoch: 13/100 Step: 700 Training Loss: 0.5527 Validation Loss: 0.5450\n",
      "Epoch: 13/100 Training Loss: 0.5422\n",
      "Epoch: 14/100 Step: 800 Training Loss: 0.6638 Validation Loss: 0.5857\n",
      "Epoch: 14/100 Training Loss: 0.5365\n",
      "Epoch: 15/100 Training Loss: 0.5109\n",
      "Epoch: 16/100 Step: 900 Training Loss: 0.4022 Validation Loss: 0.3892\n",
      "Epoch: 16/100 Training Loss: 0.5345\n",
      "Epoch: 17/100 Training Loss: 0.5249\n",
      "Epoch: 18/100 Step: 1000 Training Loss: 0.6975 Validation Loss: 0.6568\n",
      "Epoch: 18/100 Training Loss: 0.5139\n",
      "Epoch: 19/100 Step: 1100 Training Loss: 0.4574 Validation Loss: 0.4523\n",
      "Epoch: 19/100 Training Loss: 0.5023\n",
      "Epoch: 20/100 Training Loss: 0.4879\n",
      "Epoch: 21/100 Step: 1200 Training Loss: 0.4363 Validation Loss: 0.4256\n",
      "Epoch: 21/100 Training Loss: 0.4717\n",
      "Epoch: 22/100 Training Loss: 0.4678\n",
      "Epoch: 23/100 Step: 1300 Training Loss: 0.4343 Validation Loss: 0.3926\n",
      "Epoch: 23/100 Training Loss: 0.4667\n",
      "Epoch: 24/100 Training Loss: 0.4514\n",
      "Epoch: 25/100 Step: 1400 Training Loss: 0.3926 Validation Loss: 0.3996\n",
      "Epoch: 25/100 Training Loss: 0.4533\n",
      "Epoch: 26/100 Step: 1500 Training Loss: 0.4697 Validation Loss: 0.4472\n",
      "Epoch: 26/100 Training Loss: 0.4480\n",
      "Epoch: 27/100 Training Loss: 0.4690\n",
      "Epoch: 28/100 Step: 1600 Training Loss: 0.5836 Validation Loss: 0.5821\n",
      "Epoch: 28/100 Training Loss: 0.4270\n",
      "Epoch: 29/100 Training Loss: 0.4116\n",
      "Epoch: 30/100 Step: 1700 Training Loss: 0.3924 Validation Loss: 0.3759\n",
      "Epoch: 30/100 Training Loss: 0.4129\n",
      "Epoch: 31/100 Training Loss: 0.3984\n",
      "Epoch: 32/100 Step: 1800 Training Loss: 0.5534 Validation Loss: 0.5349\n",
      "Epoch: 32/100 Training Loss: 0.3825\n",
      "Epoch: 33/100 Step: 1900 Training Loss: 0.3717 Validation Loss: 0.3265\n",
      "Epoch: 33/100 Training Loss: 0.4000\n",
      "Epoch: 34/100 Training Loss: 0.3714\n",
      "Epoch: 35/100 Step: 2000 Training Loss: 0.4052 Validation Loss: 0.3743\n",
      "Epoch: 35/100 Training Loss: 0.3821\n",
      "Epoch: 36/100 Training Loss: 0.3869\n",
      "Epoch: 37/100 Step: 2100 Training Loss: 0.4763 Validation Loss: 0.4676\n",
      "Epoch: 37/100 Training Loss: 0.3715\n",
      "Epoch: 38/100 Step: 2200 Training Loss: 0.3353 Validation Loss: 0.3361\n",
      "Epoch: 38/100 Training Loss: 0.3713\n",
      "Epoch: 39/100 Training Loss: 0.3442\n",
      "Epoch: 40/100 Step: 2300 Training Loss: 0.3554 Validation Loss: 0.3240\n",
      "Epoch: 40/100 Training Loss: 0.3895\n",
      "Epoch: 41/100 Training Loss: 0.3343\n",
      "Epoch: 42/100 Step: 2400 Training Loss: 0.2500 Validation Loss: 0.2652\n",
      "Epoch: 42/100 Training Loss: 0.3398\n",
      "Epoch: 43/100 Training Loss: 0.3407\n",
      "Epoch: 44/100 Step: 2500 Training Loss: 0.2537 Validation Loss: 0.2437\n",
      "Epoch: 44/100 Training Loss: 0.3578\n",
      "Epoch: 45/100 Step: 2600 Training Loss: 0.2645 Validation Loss: 0.2592\n",
      "Epoch: 45/100 Training Loss: 0.3180\n",
      "Epoch: 46/100 Training Loss: 0.3125\n",
      "Epoch: 47/100 Step: 2700 Training Loss: 0.4844 Validation Loss: 0.4641\n",
      "Epoch: 47/100 Training Loss: 0.3076\n",
      "Epoch: 48/100 Training Loss: 0.3030\n",
      "Epoch: 49/100 Step: 2800 Training Loss: 0.2251 Validation Loss: 0.2192\n",
      "Epoch: 49/100 Training Loss: 0.2937\n",
      "Epoch: 50/100 Step: 2900 Training Loss: 0.3668 Validation Loss: 0.3521\n",
      "Epoch: 50/100 Training Loss: 0.2969\n",
      "Epoch: 51/100 Training Loss: 0.3181\n",
      "Epoch: 52/100 Step: 3000 Training Loss: 0.3063 Validation Loss: 0.3052\n",
      "Epoch: 52/100 Training Loss: 0.2771\n",
      "Epoch: 53/100 Training Loss: 0.2702\n",
      "Epoch: 54/100 Step: 3100 Training Loss: 0.2094 Validation Loss: 0.1677\n",
      "Epoch: 54/100 Training Loss: 0.2665\n",
      "Epoch: 55/100 Training Loss: 0.2592\n",
      "Epoch: 56/100 Step: 3200 Training Loss: 0.3359 Validation Loss: 0.2764\n",
      "Epoch: 56/100 Training Loss: 0.2683\n",
      "Epoch: 57/100 Step: 3300 Training Loss: 0.2028 Validation Loss: 0.1757\n",
      "Epoch: 57/100 Training Loss: 0.2418\n",
      "Epoch: 58/100 Training Loss: 0.2481\n",
      "Epoch: 59/100 Step: 3400 Training Loss: 0.2865 Validation Loss: 0.3137\n",
      "Epoch: 59/100 Training Loss: 0.2319\n",
      "Epoch: 60/100 Training Loss: 0.2317\n",
      "Epoch: 61/100 Step: 3500 Training Loss: 0.1023 Validation Loss: 0.0838\n",
      "Epoch: 61/100 Training Loss: 0.2348\n",
      "Epoch: 62/100 Training Loss: 0.2292\n",
      "Epoch: 63/100 Step: 3600 Training Loss: 0.6352 Validation Loss: 0.6438\n",
      "Epoch: 63/100 Training Loss: 0.2466\n",
      "Epoch: 64/100 Step: 3700 Training Loss: 0.1290 Validation Loss: 0.1268\n",
      "Epoch: 64/100 Training Loss: 0.2059\n",
      "Epoch: 65/100 Training Loss: 0.2365\n",
      "Epoch: 66/100 Step: 3800 Training Loss: 0.2417 Validation Loss: 0.2253\n",
      "Epoch: 66/100 Training Loss: 0.2069\n",
      "Epoch: 67/100 Training Loss: 0.1936\n",
      "Epoch: 68/100 Step: 3900 Training Loss: 0.1242 Validation Loss: 0.0870\n",
      "Epoch: 68/100 Training Loss: 0.2030\n",
      "Epoch: 69/100 Step: 4000 Training Loss: 0.3077 Validation Loss: 0.2560\n",
      "Epoch: 69/100 Training Loss: 0.2035\n",
      "Epoch: 70/100 Training Loss: 0.1960\n",
      "Epoch: 71/100 Step: 4100 Training Loss: 0.1396 Validation Loss: 0.1226\n",
      "Epoch: 71/100 Training Loss: 0.1984\n",
      "Epoch: 72/100 Training Loss: 0.1922\n",
      "Epoch: 73/100 Step: 4200 Training Loss: 0.1464 Validation Loss: 0.1716\n",
      "Epoch: 73/100 Training Loss: 0.1696\n",
      "Epoch: 74/100 Training Loss: 0.1565\n",
      "Epoch: 75/100 Step: 4300 Training Loss: 0.1130 Validation Loss: 0.1074\n",
      "Epoch: 75/100 Training Loss: 0.1633\n",
      "Epoch: 76/100 Step: 4400 Training Loss: 0.1020 Validation Loss: 0.1115\n",
      "Epoch: 76/100 Training Loss: 0.1823\n",
      "Epoch: 77/100 Training Loss: 0.1495\n",
      "Epoch: 78/100 Step: 4500 Training Loss: 0.2175 Validation Loss: 0.2081\n",
      "Epoch: 78/100 Training Loss: 0.1659\n",
      "Epoch: 79/100 Training Loss: 0.1487\n",
      "Epoch: 80/100 Step: 4600 Training Loss: 0.0789 Validation Loss: 0.0767\n",
      "Epoch: 80/100 Training Loss: 0.1469\n",
      "Epoch: 81/100 Training Loss: 0.1992\n",
      "Epoch: 82/100 Step: 4700 Training Loss: 0.1186 Validation Loss: 0.0799\n",
      "Epoch: 82/100 Training Loss: 0.1384\n",
      "Epoch: 83/100 Step: 4800 Training Loss: 0.1373 Validation Loss: 0.1147\n",
      "Epoch: 83/100 Training Loss: 0.1444\n",
      "Epoch: 84/100 Training Loss: 0.1383\n",
      "Epoch: 85/100 Step: 4900 Training Loss: 0.2452 Validation Loss: 0.2426\n",
      "Epoch: 85/100 Training Loss: 0.1446\n",
      "Epoch: 86/100 Training Loss: 0.1311\n",
      "Epoch: 87/100 Step: 5000 Training Loss: 0.0548 Validation Loss: 0.0472\n",
      "Epoch: 87/100 Training Loss: 0.1205\n",
      "Epoch: 88/100 Step: 5100 Training Loss: 0.1397 Validation Loss: 0.0748\n",
      "Epoch: 88/100 Training Loss: 0.1430\n",
      "Epoch: 89/100 Training Loss: 0.1309\n",
      "Epoch: 90/100 Step: 5200 Training Loss: 0.1077 Validation Loss: 0.0671\n",
      "Epoch: 90/100 Training Loss: 0.1099\n",
      "Epoch: 91/100 Training Loss: 0.1121\n",
      "Epoch: 92/100 Step: 5300 Training Loss: 0.1950 Validation Loss: 0.1455\n",
      "Epoch: 92/100 Training Loss: 0.1070\n",
      "Epoch: 93/100 Training Loss: 0.1814\n",
      "Epoch: 94/100 Step: 5400 Training Loss: 0.0837 Validation Loss: 0.0745\n",
      "Epoch: 94/100 Training Loss: 0.1196\n",
      "Epoch: 95/100 Step: 5500 Training Loss: 0.0747 Validation Loss: 0.0458\n",
      "Epoch: 95/100 Training Loss: 0.1101\n",
      "Epoch: 96/100 Training Loss: 0.1452\n",
      "Epoch: 97/100 Step: 5600 Training Loss: 0.1068 Validation Loss: 0.0905\n",
      "Epoch: 97/100 Training Loss: 0.0919\n",
      "Epoch: 98/100 Training Loss: 0.0964\n",
      "Epoch: 99/100 Step: 5700 Training Loss: 0.0181 Validation Loss: 0.0119\n",
      "Epoch: 99/100 Training Loss: 0.1052\n",
      "Epoch: 100/100 Step: 5800 Training Loss: 0.0500 Validation Loss: 0.0238\n",
      "Epoch: 100/100 Training Loss: 0.1034\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 100  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda:0\"\n",
    "# else:\n",
    "#     device = \"cpu\"\n",
    "# print(device)\n",
    "\n",
    "training_loss_epoches = []\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    training_loss = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         print(i, inputs.shape, labels.shape)\n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        training_loss.append(loss.item())\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            v_h = net.init_hidden(batch_size)\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()\n",
    "    training_loss_epoches.append(np.mean(training_loss))\n",
    "    print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "          \"Training Loss: {:.4f}\".format(np.mean(training_loss)))\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(training_loss_epoches))\n",
    "plt.plot(np.linspace(1, len(training_loss_epoches), len(training_loss_epoches)).astype(int), training_loss_epoches)\n",
    "torch.save(net, 'model_task2')\n",
    "torch.save(net.state_dict(), 'model_param_task2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('model_param_task2'))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.4219\n",
      "Test Accuracy: 0.68\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "net.eval()\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    try:\n",
    "        test_output, test_h = net(inputs.to(device))\n",
    "    except IndexError:\n",
    "        print(inputs)\n",
    "#     print(labels.dtype, test_output.dtype)\n",
    "#     print(inputs)\n",
    "    loss = criterion(test_output.detach().to(device), labels.float().to(device))\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds).to(device))\n",
    "    correct = np.squeeze(correct_tensor.cpu().detach().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n",
      "248\n",
      "4665\n",
      "2469\n"
     ]
    }
   ],
   "source": [
    "print(len(test_y) - np.sum(test_y))\n",
    "print(np.sum(test_y))\n",
    "print(len(encoded_labels))\n",
    "print(np.sum(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
