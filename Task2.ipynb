{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch as torch\n",
    "from torch import nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math as mt\n",
    "import time\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "import helper\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, features, embedding_size):\n",
    "        super().__init__()\n",
    "        initrange = 0.5 / embedding_size\n",
    "        self.fc1 = nn.Linear(features, embedding_size)\n",
    "        self.fc2 = nn.Linear(embedding_size, features)\n",
    "\n",
    "\n",
    "    def forward(self, one_hot):\n",
    "        x = self.fc1(one_hot.float())\n",
    "        x = self.fc2(x)\n",
    "        log_softmax = torch.nn.functional.log_softmax(x, dim=1)\n",
    "        return log_softmax\n",
    "#     pass\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "url = 'https://raw.githubusercontent.com/salmanedhi/NNTI-WS2021-NLP-Project/main/data/hindi_hatespeech.tsv'\n",
    "\n",
    "data = pd.read_csv(url, sep='\\t')\n",
    "    \n",
    "#data = pd.read_csv('data/hindi_hatespeech.tsv', sep='\\t')\n",
    "data_development = shuffle(data)\n",
    "labels = data_development['task_2']\n",
    "# data_development = data\n",
    "type(data_development['task_1'])\n",
    "\n",
    "print(\"Done\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  4665\n",
      "Total words: 141550\n",
      "Unique words: 19836\n"
     ]
    }
   ],
   "source": [
    "#Stopwords Removal\n",
    "sentences = helper.apply_stopword_removal(data_development)\n",
    "print(\"Number of sentences: \" , len(sentences))\n",
    "\n",
    "#Building Vocabulary\n",
    "V, non_unique = helper.build_vocabulary(sentences)\n",
    "print('Total words:', len(non_unique))\n",
    "print('Unique words:', len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(\n",
      "  (fc1): Linear(in_features=19836, out_features=600, bias=True)\n",
      "  (fc2): Linear(in_features=600, out_features=19836, bias=True)\n",
      ")\n",
      "torch.Size([19837, 600]) torch.Size([600, 19836])\n"
     ]
    }
   ],
   "source": [
    "#Load Word2Vec embeddings modu\n",
    "model = Word2Vec(19836, 600)\n",
    "model.cpu()\n",
    "model.load_state_dict(torch.load('model_param_finalised', map_location=torch.device('cpu')))\n",
    "\n",
    "print(model.eval())\n",
    "weights1 = torch.transpose(model.fc1.weight, 0, 1)\n",
    "weights2 = torch.transpose(model.fc2.weight, 0, 1)\n",
    "temp_row = np.zeros(600).reshape(1,600)\n",
    "weights1_np = weights1.cpu().detach().numpy()\n",
    "weights1_np = np.concatenate((temp_row, weights1_np), axis=0)\n",
    "weights1 = torch.Tensor(weights1_np)\n",
    "print(weights1.shape, weights2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4665, 132)\n",
      "[1 1 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "## create number array of sentences (replace each word with each numeric value)\n",
    "x_data, max_len_curr = helper.sentence_to_numeric_arr(sentences, V)\n",
    "\n",
    "## apply padding\n",
    "padded = np.array(helper.padding(x_data, max_len_curr))\n",
    "\n",
    "# x_data = x_data.astype('float32')\n",
    "# print(x_data.shape)\n",
    "print(padded.shape)\n",
    "encoded_labels = [0 if label == \"NONE\" else 1 for label in labels]\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "print(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done (4665, 132) (466, 132) (3732, 132)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "##############  7. SPLIT DATA & GET (REVIEW, LABEL) DATALOADER  ###############\n",
    "###############################################################################\n",
    "train_ratio = 0.8\n",
    "valid_ratio = (1 - train_ratio)/2\n",
    "total = padded.shape[0]\n",
    "train_cutoff = int(total * train_ratio)\n",
    "valid_cutoff = int(total * (1 - valid_ratio))\n",
    "\n",
    "train_x, train_y = padded[:train_cutoff], encoded_labels[:train_cutoff]\n",
    "valid_x, valid_y = padded[train_cutoff : valid_cutoff], encoded_labels[train_cutoff : valid_cutoff]\n",
    "test_x, test_y = padded[valid_cutoff:], encoded_labels[valid_cutoff:]\n",
    "# print(train_x.shape, train_y.shape)\n",
    "# print(test_x.shape, test_y.shape)\n",
    "# print(valid_x.shape, valid_y.shape)\n",
    "# train_x = torch.from_numpy(train_x)\n",
    "train_data = TensorDataset(torch.tensor(train_x), torch.tensor(train_y))\n",
    "valid_data = TensorDataset(torch.tensor(valid_x), torch.tensor(valid_y))\n",
    "test_data = TensorDataset(torch.tensor(test_x), torch.tensor(test_y))\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = True, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True, drop_last=True)\n",
    "print(\"Done\", padded.shape, valid_x.shape, train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_vocab, n_embed, n_hidden, n_output, n_layers, drop_p = 0.5):\n",
    "    \n",
    "#     def __init__(self, cool):\n",
    "        super().__init__()\n",
    "        # params: \"n_\" means dimension\n",
    "        self.n_vocab = n_vocab     # number of unique words in vocabulary\n",
    "        self.n_layers = n_layers   # number of LSTM layers \n",
    "        self.n_hidden = n_hidden   # number of hidden nodes in LSTM\n",
    "        \n",
    "        self.embedding = nn.Embedding(n_vocab, n_embed)\n",
    "        #embeddings is a torch tensor.\n",
    "        self.embedding.weight = nn.Parameter(weights1, requires_grad = False)\n",
    "        self.lstm = nn.LSTM(n_embed, n_hidden, n_layers, batch_first = True, dropout = drop_p)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc = nn.Linear(n_hidden, n_output)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward (self, input_words):\n",
    "                                             # INPUT   :  (batch_size, seq_length)\n",
    "        embedded_words = self.embedding(input_words)    # (batch_size, seq_length, n_embed)\n",
    "        lstm_out, h = self.lstm(embedded_words)         # (batch_size, seq_length, n_hidden)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.n_hidden) # (batch_size*seq_length, n_hidden)\n",
    "        fc_out = self.fc(lstm_out)                      # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = self.sigmoid(fc_out)              # (batch_size*seq_length, n_output)\n",
    "        sigmoid_out = sigmoid_out.view(batch_size, -1)  # (batch_size, seq_length*n_output)\n",
    "#         print('HEre',input_words.shape)\n",
    "        # extract the output of ONLY the LAST output of the LAST element of the sequence\n",
    "        sigmoid_last = sigmoid_out[:, -1]               # (batch_size, 1)\n",
    "        \n",
    "        return sigmoid_last, h\n",
    "    \n",
    "    \n",
    "    def init_hidden (self, batch_size):  # initialize hidden weights (h,c) to 0\n",
    "        \n",
    "#         device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        weights = next(self.parameters()).data\n",
    "        h = (weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "             weights.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19837 600 32 1 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(19837, 600)\n",
       "  (lstm): LSTM(600, 32, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_vocab = len(V) + 1\n",
    "n_embed = 600\n",
    "n_hidden = 32\n",
    "n_output = 1   # 1 (\"positive\") or 0 (\"negative\")\n",
    "n_layers = 2\n",
    "print(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net = SentimentLSTM(n_vocab, n_embed, n_hidden, n_output, n_layers)\n",
    "net.to(device)\n",
    "# net = SentimentLSTM(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 100  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     device = \"cuda:0\"\n",
    "# else:\n",
    "#     device = \"cpu\"\n",
    "# print(device)\n",
    "\n",
    "training_loss_epoches = []\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size)\n",
    "    training_loss = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         print(i, inputs.shape, labels.shape)\n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        training_loss.append(loss.item())\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (step % print_every) == 0:            \n",
    "            ######################\n",
    "            ##### VALIDATION #####\n",
    "            ######################\n",
    "            net.eval()\n",
    "            valid_losses = []\n",
    "            v_h = net.init_hidden(batch_size)\n",
    "            \n",
    "            for v_inputs, v_labels in valid_loader:\n",
    "                v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "                v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "                v_output, v_h = net(v_inputs)\n",
    "                v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "                valid_losses.append(v_loss.item())\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "                  \"Step: {}\".format(step),\n",
    "                  \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "                  \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "            net.train()\n",
    "    training_loss_epoches.append(np.mean(training_loss))\n",
    "    print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "          \"Training Loss: {:.4f}\".format(np.mean(training_loss)))\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_loss_epoches))\n",
    "plt.plot(np.linspace(1, len(training_loss_epoches), len(training_loss_epoches)).astype(int), training_loss_epoches)\n",
    "torch.save(net, 'model_task2')\n",
    "torch.save(net.state_dict(), 'model_param_task2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentLSTM(\n",
       "  (embedding): Embedding(19837, 600)\n",
       "  (lstm): LSTM(600, 32, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load('model_param_task2'))\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.3468\n",
      "Test Accuracy: 0.50\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "net.eval()\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    try:\n",
    "        test_output, test_h = net(inputs.to(device))\n",
    "    except IndexError:\n",
    "        print(inputs)\n",
    "#     print(labels.dtype, test_output.dtype)\n",
    "#     print(inputs)\n",
    "    loss = criterion(test_output.detach().to(device), labels.float().to(device))\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds).to(device))\n",
    "    correct = np.squeeze(correct_tensor.cpu().detach().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n",
      "250\n",
      "4665\n",
      "2469\n"
     ]
    }
   ],
   "source": [
    "print(len(test_y) - np.sum(test_y))\n",
    "print(np.sum(test_y))\n",
    "print(len(encoded_labels))\n",
    "print(np.sum(encoded_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  hate       category\n",
      "12216  ‡¶≠‡¶æ‡¶á ‡¶™‡¶æ‡¶™‡¶®‡ßá‡¶∞ ‡¶ï‡ßá‡¶∏‡¶ø‡¶®‡ßã ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶ü‡¶æ ‡¶®‡¶ø‡ßü‡ßá ‡¶≠‡¶ø‡¶°‡¶ø‡¶ì ‡¶¨‡¶æ‡¶®‡¶æ‡¶® ‡¶™‡ßç...     0         sports\n",
      "1155                    ‡¶ö‡¶æ‡¶Æ‡¶ö‡¶æ ‡¶∂‡¶æ‡¶≤‡¶æ‡¶∞‡¶æ ‡¶§‡ßã‡¶Æ‡¶∞‡¶æ ‡¶è‡¶∞‡¶ï‡¶Æ ‡¶ï‡¶∞‡¶ö ‡¶ï‡ßá‡¶®‡•§     1         sports\n",
      "2254                           ‡¶®‡¶¶‡ßÄ‡¶∞ ‡¶®‡¶æ‡¶≠‡ßÄ‡¶∞ ‡¶π‡ßÅ‡¶≤‡¶æ‡¶ï ‡¶ü‡¶æ ‡¶∏‡ßá‡¶∞‡¶ï‡¶Æ     1  entertainment\n",
      "2252   ‡¶¶‡¶æ‡¶°‡¶º‡¶ø‡ßü‡¶æ‡¶≤‡¶æ ‡¶è‡¶á ‡¶Æ‡¶æ‡¶¶‡¶æ‡¶∞‡¶ö‡ßã‡¶¶ ‡¶¨‡¶ø‡¶ö‡¶æ‡¶∞‡¶ï‡¶ü‡¶æ ‡¶ï‡ßá ‡¶π‡¶æ‡¶≤‡¶æ ‡¶ï‡ßã‡¶•‡¶æ ‡¶•‡ßá...     1  entertainment\n",
      "12166                                   ‡¶™‡¶æ‡¶§‡¶®‡ßã ‡¶ñ‡ßá‡¶≤‡¶æ ‡¶è‡¶ó‡ßÅ‡¶≤‡ßã     0         sports\n",
      "...                                                  ...   ...            ...\n",
      "10183     ‡¶Ü‡¶Æ‡¶ø ‡¶ö‡¶æ‡¶á ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡¶ø ‡¶ï‡ßã‡¶ö ‡¶ö‡¶æ‡¶á ‡¶¨‡¶ø‡¶¶‡ßá‡¶∂‡¶ø ‡¶ï‡ßã‡¶ö ‡¶¶‡¶∞‡¶ï‡¶æ‡¶∞ ‡¶®‡¶æ‡¶á     0         sports\n",
      "11614                        ‡¶Ü‡¶≤‡ßç‡¶≤‡¶æ‡¶π‡ßç ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶¶‡ßá‡¶∞ ‡¶¨‡¶ø‡¶ö‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ     0         sports\n",
      "494    ‡¶è‡¶á ‡¶∏‡¶æ‡¶≤‡¶æ‡¶∞ ‡¶¨‡¶æ‡ßú‡¶ø ‡¶ï‡¶á‡•§‡¶¨‡¶≤‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßá‡¶® ‡¶ï‡ßá‡¶â‡•§‡¶∏‡¶æ‡¶≤‡¶æ‡¶∞‡ßá ‡¶ú‡ßÅ‡¶§‡¶æ ‡¶Æ...     1         sports\n",
      "12120                  ‡¶è‡¶ï ‡¶¶‡¶´‡¶æ ‡¶è‡¶ï ‡¶¶‡¶æ‡¶¨‡¶ø ‡¶™‡¶æ‡¶™‡¶®‡ßá‡¶∞ ‡¶™‡¶¶‡¶§‡ßç‡¶Ø‡¶æ‡¶ó ‡¶ö‡¶æ‡¶á     0         sports\n",
      "10706   ‡¶ï‡¶ø‡¶õ‡ßÅ ‡¶¨‡¶≤‡¶æ‡¶∞ ‡¶®‡¶æ‡¶á ‡¶Ö‡¶Æ‡¶ø‡¶§ ‡¶≠‡¶æ‡¶á‡•§‡¶ï‡¶•‡¶æ ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶è‡¶ï‡¶¶‡¶Æ ‡¶¶‡¶ø‡¶≤‡ßá ‡¶≤‡¶æ‡¶ó‡ßá‡•§üò∞     0         sports\n",
      "\n",
      "[4665 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Reading Bengali data\n",
    "bengali_data = pd.read_csv('data/bengali_hatespeech.csv')\n",
    "\n",
    "bengali_data_hate = bengali_data.loc[bengali_data['hate'] == 1]\n",
    "bengali_data_not_hate = bengali_data.loc[bengali_data['hate'] == 0]\n",
    "\n",
    "bengali_data_hate = bengali_data_hate.iloc[0:2332] \n",
    "bengali_data_not_hate = bengali_data_not_hate.iloc[0:2333]\n",
    "\n",
    "bengali_data = pd.concat([bengali_data_hate, bengali_data_not_hate])\n",
    "bengali_data = shuffle(bengali_data)\n",
    "bengali_data.columns = [\"text\", \"hate\", \"category\"]\n",
    "\n",
    "labels = bengali_data['hate']\n",
    "labels = np.array(labels)\n",
    "print(bengali_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  4665\n",
      "Total words: 64027\n",
      "Unique words: 14482\n"
     ]
    }
   ],
   "source": [
    "## Stopwords Removal of Bengali Data\n",
    "bengali_sentences = helper.apply_stopword_removal(bengali_data)\n",
    "print(\"Number of sentences: \" , len(bengali_sentences))\n",
    "\n",
    "## Building Vocabulary\n",
    "bengali_V, bengali_non_unique = helper.build_vocabulary(bengali_sentences)\n",
    "print('Total words:', len(bengali_non_unique))\n",
    "print('Unique words:', len(bengali_V))\n",
    "\n",
    "## Sentence to numeric array\n",
    "x_data_bengali, max_len_curr = helper.sentence_to_numeric_arr(bengali_sentences, bengali_V)\n",
    "\n",
    "## Apply Padding\n",
    "padded = np.array(helper.padding(x_data_bengali, max_len_curr))\n",
    "\n",
    "test_data = TensorDataset(torch.tensor(padded), torch.tensor(labels))\n",
    "\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9384\n",
      "Test Accuracy: 0.49\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    try:\n",
    "        test_output, test_h = net(inputs.to(device))\n",
    "    except IndexError:\n",
    "        print(inputs)\n",
    "#     print(labels.dtype, test_output.dtype)\n",
    "#     print(inputs)\n",
    "    loss = criterion(test_output.detach().to(device), labels.float().to(device))\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds).to(device))\n",
    "    correct = np.squeeze(correct_tensor.cpu().detach().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
