{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch as torch\n",
    "from torch import nn\n",
    "from nltk.tokenize import word_tokenize\n",
    "import math as mt\n",
    "import time\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data as data_utils\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.utils import shuffle\n",
    "import helper\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1: Classifier on Hindi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Reading Hindi Data\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dev = \"cpu\"\n",
    "    \n",
    "url = 'data/hindi_hatespeech.tsv'\n",
    "data = pd.read_csv(url, sep='\\t')\n",
    "    \n",
    "data_development = shuffle(data)\n",
    "labels = data_development['task_2']\n",
    "# data_development = data\n",
    "type(data_development['task_1'])\n",
    "\n",
    "print(\"Done\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  4665\n",
      "Total words: 141550\n",
      "Unique words: 19836\n"
     ]
    }
   ],
   "source": [
    "#Removing stop words\n",
    "sentences = helper.apply_stopword_removal(data_development)\n",
    "print(\"Number of sentences: \" , len(sentences))\n",
    "\n",
    "#Building Vocabulary\n",
    "V, non_unique = helper.build_vocabulary(sentences)\n",
    "print('Total words:', len(non_unique))\n",
    "print('Unique words:', len(V))\n",
    "embedding_size = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(\n",
      "  (fc1): Linear(in_features=19836, out_features=640, bias=True)\n",
      "  (fc2): Linear(in_features=640, out_features=19836, bias=True)\n",
      ")\n",
      "torch.Size([19837, 640]) torch.Size([640, 19836])\n"
     ]
    }
   ],
   "source": [
    "#Load Word2Vec Hindi embeddings module\n",
    "weights1, weights2 = helper.load_word2vec_embeddings('model_param_hindi', device, len(V), embedding_size)\n",
    "#weights1 contain the trained embedding weights from task1\n",
    "print(weights1.shape, weights2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded Data Shape:  (4665, 132)\n",
      "Encoded Labels Shape:  (4665,)\n"
     ]
    }
   ],
   "source": [
    "## create number array of sentences (replace each word with each numeric value)\n",
    "x_data, max_len_curr = helper.sentence_to_numeric_arr(sentences, V)\n",
    "\n",
    "## apply padding\n",
    "padded = np.array(helper.padding(x_data, max_len_curr))\n",
    "\n",
    "print(\"Padded Data Shape: \", padded.shape)\n",
    "encoded_labels = [0 if label == \"NONE\" else 1 for label in labels]\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "print(\"Encoded Labels Shape: \",encoded_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#split data into train & test set\n",
    "batch_size = 64\n",
    "train_loader, test_loader = helper.split_data_train_valid_test(padded, encoded_labels, batch_size)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Hindi Classfier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-04a06d16b05a>:26: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 Training Loss: 0.6797\n",
      "Epoch: 2/20 Training Loss: 0.4752\n",
      "Epoch: 3/20 Training Loss: 0.2064\n",
      "Epoch: 4/20 Training Loss: 0.0895\n",
      "Epoch: 5/20 Training Loss: 0.0495\n",
      "Epoch: 6/20 Training Loss: 0.0312\n",
      "Epoch: 7/20 Training Loss: 0.0305\n",
      "Epoch: 8/20 Training Loss: 0.0195\n",
      "Epoch: 9/20 Training Loss: 0.0235\n",
      "Epoch: 10/20 Training Loss: 0.0157\n",
      "Epoch: 11/20 Training Loss: 0.0118\n",
      "Epoch: 12/20 Training Loss: 0.0098\n",
      "Epoch: 13/20 Training Loss: 0.0099\n",
      "Epoch: 14/20 Training Loss: 0.0111\n",
      "Epoch: 15/20 Training Loss: 0.0078\n",
      "Epoch: 16/20 Training Loss: 0.0102\n",
      "Epoch: 17/20 Training Loss: 0.0098\n",
      "Epoch: 18/20 Training Loss: 0.0057\n",
      "Epoch: 19/20 Training Loss: 0.0059\n",
      "Epoch: 20/20 Training Loss: 0.0052\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize LSTM Model\n",
    "net, criterion = helper.initialize_SentimentLSTM_model_task3(len(V) + 1, batch_size, embedding_size, 32, 1, 2, device, weights1)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001, amsgrad=True)\n",
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 20\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM\n",
    "\n",
    "\n",
    "training_loss_epoches = []\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size, device)\n",
    "    training_loss = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs, batch_size)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        training_loss.append(loss.item())\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if (step % print_every) == 0:            \n",
    "#             ######################\n",
    "#             ##### VALIDATION #####\n",
    "#             ######################\n",
    "#             net.eval()\n",
    "#             valid_losses = []\n",
    "#             v_h = net.init_hidden(batch_size, device)\n",
    "            \n",
    "#             for v_inputs, v_labels in valid_loader:\n",
    "#                 v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "#                 v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "#                 v_output, v_h = net(v_inputs, batch_size)\n",
    "#                 v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "#                 valid_losses.append(v_loss.item())\n",
    "\n",
    "#             print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "#                   \"Step: {}\".format(step),\n",
    "#                   \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "#                   \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "#             net.train()\n",
    "    training_loss_epoches.append(np.mean(training_loss))\n",
    "    print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "          \"Training Loss: {:.4f}\".format(np.mean(training_loss)))\n",
    "plt.plot(np.linspace(1, len(training_loss_epoches), len(training_loss_epoches)).astype(int), training_loss_epoches)\n",
    "plt.savefig('Task3_hindi.png') \n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Hindi Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1328\n",
      "Test Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size, device)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    try:\n",
    "        test_output, test_h = net(inputs.to(device), batch_size)\n",
    "    except IndexError:\n",
    "        print(inputs)\n",
    "#     print(labels.dtype, test_output.dtype)\n",
    "#     print(inputs)\n",
    "    loss = criterion(test_output.detach().to(device), labels.float().to(device))\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds).to(device))\n",
    "    correct = np.squeeze(correct_tensor.cpu().detach().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.2: Testing Bengali Data on Hindi Classifier (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4665, 3) (4665,)\n",
      "Number of sentences:  4665\n",
      "Total words: 64027\n",
      "Unique words: 14482\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Reading Bengali data\n",
    "bengali_data, labels = helper.get_bengali_data('data/bengali_hatespeech.csv')\n",
    "\n",
    "print(bengali_data.shape, labels.shape)\n",
    "bengali_sentences = helper.apply_stopword_removal(bengali_data)\n",
    "print(\"Number of sentences: \" , len(bengali_sentences))\n",
    "\n",
    "## Building Vocabulary\n",
    "bengali_V, bengali_non_unique = helper.build_vocabulary(bengali_sentences)\n",
    "print('Total words:', len(bengali_non_unique))\n",
    "print('Unique words:', len(bengali_V))\n",
    "\n",
    "## Sentence to numeric array\n",
    "x_data_bengali, max_len_curr = helper.sentence_to_numeric_arr(bengali_sentences, bengali_V)\n",
    "\n",
    "## Apply Padding\n",
    "padded = np.array(helper.padding(x_data_bengali, max_len_curr))\n",
    "\n",
    "## Splitting data into train and test\n",
    "train_loader, test_loader = helper.split_data_train_valid_test(padded, labels, 64)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifier on Bengali data with trained Hindi embeddings (Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-fcf209974ed7>:25: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30 Training Loss: 0.6884\n",
      "Epoch: 2/30 Training Loss: 0.5121\n",
      "Epoch: 3/30 Training Loss: 0.2281\n",
      "Epoch: 4/30 Training Loss: 0.1111\n",
      "Epoch: 5/30 Training Loss: 0.0707\n",
      "Epoch: 6/30 Training Loss: 0.0484\n",
      "Epoch: 7/30 Training Loss: 0.0410\n",
      "Epoch: 8/30 Training Loss: 0.0292\n",
      "Epoch: 9/30 Training Loss: 0.0208\n",
      "Epoch: 10/30 Training Loss: 0.0159\n",
      "Epoch: 11/30 Training Loss: 0.0127\n",
      "Epoch: 12/30 Training Loss: 0.0115\n",
      "Epoch: 13/30 Training Loss: 0.0080\n",
      "Epoch: 14/30 Training Loss: 0.0071\n",
      "Epoch: 15/30 Training Loss: 0.0064\n",
      "Epoch: 16/30 Training Loss: 0.0058\n",
      "Epoch: 17/30 Training Loss: 0.0051\n",
      "Epoch: 18/30 Training Loss: 0.0052\n",
      "Epoch: 19/30 Training Loss: 0.0047\n",
      "Epoch: 20/30 Training Loss: 0.0063\n",
      "Epoch: 21/30 Training Loss: 0.0038\n",
      "Epoch: 22/30 Training Loss: 0.0035\n",
      "Epoch: 23/30 Training Loss: 0.0082\n",
      "Epoch: 24/30 Training Loss: 0.0251\n",
      "Epoch: 25/30 Training Loss: 0.0184\n",
      "Epoch: 26/30 Training Loss: 0.0187\n",
      "Epoch: 27/30 Training Loss: 0.0111\n",
      "Epoch: 28/30 Training Loss: 0.0066\n",
      "Epoch: 29/30 Training Loss: 0.0042\n",
      "Epoch: 30/30 Training Loss: 0.0049\n",
      "Done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "net, criterion = helper.initialize_SentimentLSTM_model_task3_bengali(len(V) + 1, batch_size, embedding_size, 32, 1, 2, device, weights1)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001, amsgrad=True)\n",
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 30  # validation loss increases from ~ epoch 3 or 4\n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "\n",
    "\n",
    "training_loss_epoches = []\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size, device)\n",
    "    training_loss = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         print(i, inputs.shape, labels.shape)\n",
    "        # making requires_grad = False for the latest set of h\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs, batch_size)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        training_loss.append(loss.item())\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if (step % print_every) == 0:            \n",
    "#             ######################\n",
    "#             ##### VALIDATION #####\n",
    "#             ######################\n",
    "#             net.eval()\n",
    "#             valid_losses = []\n",
    "#             v_h = net.init_hidden(batch_size, device)\n",
    "            \n",
    "#             for v_inputs, v_labels in valid_loader:\n",
    "#                 v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "#                 v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "#                 v_output, v_h = net(v_inputs, batch_size)\n",
    "#                 v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "#                 valid_losses.append(v_loss.item())\n",
    "\n",
    "#             print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "#                   \"Step: {}\".format(step),\n",
    "#                   \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "#                   \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "#             net.train()\n",
    "    training_loss_epoches.append(np.mean(training_loss))\n",
    "    print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "          \"Training Loss: {:.4f}\".format(np.mean(training_loss)))\n",
    "\n",
    "plt.plot(np.linspace(1, len(training_loss_epoches), len(training_loss_epoches)).astype(int), training_loss_epoches)\n",
    "plt.savefig(\"Task3_transfer_learning.png\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.5072\n",
      "Test Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size, device)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    try:\n",
    "        test_output, test_h = net(inputs.to(device), batch_size)\n",
    "    except IndexError:\n",
    "        print(inputs)\n",
    "#     print(labels.dtype, test_output.dtype)\n",
    "#     print(inputs)\n",
    "    loss = criterion(test_output.detach().to(device), labels.float().to(device))\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds).to(device))\n",
    "    correct = np.squeeze(correct_tensor.cpu().detach().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.3: Training classifier on Bengali embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Bengali Data\n",
    "bengali_data = pd.read_csv('data/bengali_hatespeech.csv')\n",
    "\n",
    "bengali_data_hate = bengali_data.loc[bengali_data['hate'] == 1]\n",
    "bengali_data_not_hate = bengali_data.loc[bengali_data['hate'] == 0]\n",
    "\n",
    "bengali_data_hate = bengali_data_hate.iloc[0:2332] \n",
    "bengali_data_not_hate = bengali_data_not_hate.iloc[0:2333]\n",
    "\n",
    "data_development = pd.concat([bengali_data_hate, bengali_data_not_hate])\n",
    "data_development = shuffle(data_development)\n",
    "data_development.columns = [\"text\", \"hate\", \"category\"]\n",
    "\n",
    "labels = data_development['hate']\n",
    "labels = np.array(labels)\n",
    "\n",
    "#Removing stop words\n",
    "sentences = helper.apply_stopword_removal(data_development)\n",
    "print(\"Number of sentences: \" , len(sentences))\n",
    "\n",
    "#Building Vocabulary\n",
    "V, non_unique = helper.build_vocabulary(sentences)\n",
    "print('Total words:', len(non_unique))\n",
    "print('Unique words:', len(V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "window_size = 4\n",
    "embedding_size = 640\n",
    "learning_rate = 0.001\n",
    "epochs =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model \n",
    "\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, features, embedding_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(features, embedding_size)\n",
    "        self.fc2 = nn.Linear(embedding_size, features)\n",
    "\n",
    "\n",
    "    def forward(self, one_hot):\n",
    "        x = self.fc1(one_hot.float())\n",
    "        x = self.fc2(x)\n",
    "        log_softmax = torch.nn.functional.log_softmax(x, dim=1)\n",
    "        return log_softmax\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating target, context tuple\n",
    "x_train, y_train = helper.create_dataset(sentences, non_unique, V, window_size)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('x_train_bengali.npy')\n",
    "y_train = np.load('y_train_bengali.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "x_train = torch.from_numpy(x_train)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "x_train = data_utils.TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(x_train, batch_size=batch_size, shuffle=True)\n",
    "features = len(V)\n",
    "total_rows = len(y_train)\n",
    "print(total_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Bengali Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss\n",
    "net = Word2Vec(len(V), embedding_size)\n",
    "net.to(dev)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train():\n",
    "    print(\"Started:\")\n",
    "    loss_vals=  []\n",
    "    for e in range(epochs):\n",
    "        epoch_loss= []\n",
    "        for i, (X_batch, y_batch) in enumerate(train_dl):\n",
    "            X_batch = np.eye(len(V))[X_batch]\n",
    "            y_batch = np.eye(len(V))[y_batch]\n",
    "            X_batch = torch.Tensor(X_batch).to(dev)\n",
    "            y_batch = torch.Tensor(y_batch).to(dev)\n",
    "            y_pred_torch = net(X_batch)\n",
    "            loss = criterion(y_pred_torch, torch.max(torch.tensor(y_batch), 1)[1])\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            epoch_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "        loss_vals.append(sum(epoch_loss)/len(epoch_loss))\n",
    "        print('Epoch:', e,'\\tLoss:',loss_vals[e])\n",
    "#         break\n",
    "    plt.plot(np.linspace(1, epochs, epochs).astype(int), loss_vals)\n",
    "    plt.savefig('Task1_benali.png')\n",
    "    torch.save(net.state_dict(), 'model_param_bengali')\n",
    "train()\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Word2Vec embeddings module\n",
    "weights1, weights2 = helper.load_word2vec_embeddings('model_param_bengali', device, len(bengali_V), embedding_size)\n",
    "#weights1 contain the trained embedding weights from task1\n",
    "print(weights1.shape, weights2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-dc5aab19ce58>:23: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(net.parameters(), clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50 Training Loss: 0.6895\n",
      "Epoch: 2/50 Training Loss: 0.5946\n",
      "Epoch: 3/50 Training Loss: 0.3771\n",
      "Epoch: 4/50 Training Loss: 0.2356\n",
      "Epoch: 5/50 Training Loss: 0.1515\n",
      "Epoch: 6/50 Training Loss: 0.1064\n",
      "Epoch: 7/50 Training Loss: 0.0791\n",
      "Epoch: 8/50 Training Loss: 0.0661\n",
      "Epoch: 9/50 Training Loss: 0.0579\n",
      "Epoch: 10/50 Training Loss: 0.0411\n",
      "Epoch: 11/50 Training Loss: 0.0413\n",
      "Epoch: 12/50 Training Loss: 0.0342\n",
      "Epoch: 13/50 Training Loss: 0.0290\n",
      "Epoch: 14/50 Training Loss: 0.0206\n",
      "Epoch: 15/50 Training Loss: 0.0243\n",
      "Epoch: 16/50 Training Loss: 0.0210\n",
      "Epoch: 17/50 Training Loss: 0.0177\n",
      "Epoch: 18/50 Training Loss: 0.0166\n",
      "Epoch: 19/50 Training Loss: 0.0180\n",
      "Epoch: 20/50 Training Loss: 0.0222\n",
      "Epoch: 21/50 Training Loss: 0.0145\n",
      "Epoch: 22/50 Training Loss: 0.0177\n",
      "Epoch: 23/50 Training Loss: 0.0154\n",
      "Epoch: 24/50 Training Loss: 0.0098\n",
      "Epoch: 25/50 Training Loss: 0.0095\n",
      "Epoch: 26/50 Training Loss: 0.0088\n",
      "Epoch: 27/50 Training Loss: 0.0120\n",
      "Epoch: 28/50 Training Loss: 0.0094\n",
      "Epoch: 29/50 Training Loss: 0.0093\n",
      "Epoch: 30/50 Training Loss: 0.0103\n",
      "Epoch: 31/50 Training Loss: 0.0076\n",
      "Epoch: 32/50 Training Loss: 0.0106\n",
      "Epoch: 33/50 Training Loss: 0.0074\n",
      "Epoch: 34/50 Training Loss: 0.0075\n",
      "Epoch: 35/50 Training Loss: 0.0066\n",
      "Epoch: 36/50 Training Loss: 0.0086\n",
      "Epoch: 37/50 Training Loss: 0.0082\n",
      "Epoch: 38/50 Training Loss: 0.0063\n",
      "Epoch: 39/50 Training Loss: 0.0066\n",
      "Epoch: 40/50 Training Loss: 0.0046\n",
      "Epoch: 41/50 Training Loss: 0.0066\n",
      "Epoch: 42/50 Training Loss: 0.0073\n",
      "Epoch: 43/50 Training Loss: 0.0047\n",
      "Epoch: 44/50 Training Loss: 0.0094\n",
      "Epoch: 45/50 Training Loss: 0.0072\n",
      "Epoch: 46/50 Training Loss: 0.0080\n",
      "Epoch: 47/50 Training Loss: 0.0084\n",
      "Epoch: 48/50 Training Loss: 0.0065\n"
     ]
    }
   ],
   "source": [
    "net, criterion = helper.initialize_SentimentLSTM_model_task3_bengali(len(bengali_V) + 1, batch_size, embedding_size, 32, 1, 2, device, weights1)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.001, amsgrad=True)\n",
    "print_every = 100\n",
    "step = 0\n",
    "n_epochs = 50 \n",
    "clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM\n",
    "\n",
    "\n",
    "training_loss_epoches = []\n",
    "for epoch in range(n_epochs):\n",
    "    h = net.init_hidden(batch_size, device)\n",
    "    training_loss = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        step += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        h = tuple([each.data for each in h])   \n",
    "        \n",
    "        net.zero_grad()\n",
    "        output, h = net(inputs, batch_size)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        training_loss.append(loss.item())\n",
    "        nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "#         if (step % print_every) == 0:            \n",
    "#             ######################\n",
    "#             ##### VALIDATION #####\n",
    "#             ######################\n",
    "#             net.eval()\n",
    "#             valid_losses = []\n",
    "#             v_h = net.init_hidden(batch_size, device)\n",
    "            \n",
    "#             for v_inputs, v_labels in valid_loader:\n",
    "#                 v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "#                 v_h = tuple([each.data for each in v_h])\n",
    "                \n",
    "#                 v_output, v_h = net(v_inputs, batch_size)\n",
    "#                 v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "#                 valid_losses.append(v_loss.item())\n",
    "\n",
    "#             print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "#                   \"Step: {}\".format(step),\n",
    "#                   \"Training Loss: {:.4f}\".format(loss.item()),\n",
    "#                   \"Validation Loss: {:.4f}\".format(np.mean(valid_losses)))\n",
    "#             net.train()\n",
    "    training_loss_epoches.append(np.mean(training_loss))\n",
    "    print(\"Epoch: {}/{}\".format((epoch+1), n_epochs),\n",
    "          \"Training Loss: {:.4f}\".format(np.mean(training_loss)))\n",
    "\n",
    "plt.plot(np.linspace(1, len(training_loss_epoches), len(training_loss_epoches)).astype(int), training_loss_epoches)\n",
    "plt.savefig(\"Task3_bengali.png\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.5006\n",
      "Test Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "# import os \n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "net.eval()\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "test_h = net.init_hidden(batch_size, device)\n",
    "\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    test_h = tuple([each.data for each in test_h])\n",
    "    try:\n",
    "        test_output, test_h = net(inputs.to(device), batch_size)\n",
    "    except IndexError:\n",
    "        print(inputs)\n",
    "#     print(labels.dtype, test_output.dtype)\n",
    "#     print(inputs)\n",
    "    loss = criterion(test_output.detach().to(device), labels.float().to(device))\n",
    "    test_losses.append(loss.item())\n",
    "    \n",
    "    preds = torch.round(test_output.squeeze())\n",
    "    \n",
    "    \n",
    "    correct_tensor = preds.eq(labels.float().view_as(preds).to(device))\n",
    "    correct = np.squeeze(correct_tensor.cpu().detach().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "    \n",
    "print(\"Test Loss: {:.4f}\".format(np.mean(test_losses)))\n",
    "print(\"Test Accuracy: {:.2f}\".format(num_correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperopt Implementation\n",
    "### Note: Hyperopt was used for the selection of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hyperopt(space)\n",
    "    learning_rate = space['learning_rate']\n",
    "    n_epochs = space['epochs']\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate, amsgrad=True)\n",
    "    clip = 5  # for gradient clip to prevent exploding gradient problem in LSTM/RNN\n",
    "    # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "    training_loss_epoches = []\n",
    "    for epoch in range(n_epochs):\n",
    "        h = net.init_hidden(batch_size, device)\n",
    "        training_loss = []\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            h = tuple([each.data for each in h])\n",
    "            net.zero_grad()\n",
    "            output, h = net(inputs, batch_size)\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            training_loss.append(loss.item())\n",
    "            nn.utils.clip_grad_norm(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "    net.eval()\n",
    "    valid_losses = []\n",
    "    v_h = net.init_hidden(batch_size, device)\n",
    "\n",
    "    for v_inputs, v_labels in valid_loader:\n",
    "        v_inputs, v_labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        v_h = tuple([each.data for each in v_h])\n",
    "\n",
    "        v_output, v_h = net(v_inputs, batch_size)\n",
    "        v_loss = criterion(v_output.squeeze(), v_labels.float())\n",
    "        valid_losses.append(v_loss.item())\n",
    "        \n",
    "        np.mean(valid_losses)\n",
    "        return {'loss': np.mean(valid_losses), 'model': net, 'status': STATUS_OK}\n",
    "\n",
    "space = {\n",
    "        'learning_rate': hp.quniform('learning_rate', 0.001, 0.04, 0.005),\n",
    "        'epochs': hp.quniform('epochs', 20, 200, 20),\n",
    "        'train': train_loader,\n",
    "        'y_train': y_train.to_numpy(),\n",
    "        'val': val_loader}\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=train_hyperopt,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=int(max_evals),\n",
    "                trials=trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
